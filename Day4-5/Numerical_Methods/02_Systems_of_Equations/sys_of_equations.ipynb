{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll be using several functions from scipy.linalg here.\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Systems of Equations\n",
    "\n",
    "Let's start with a simple example, where we want to solve the following equations simultaneously:\n",
    "\n",
    "$$\n",
    "3x_1 + 5x_2 + 2x_3 = 11\\\\\n",
    "2x_1 - 3x_2 - 3x_3 = -1\\\\\n",
    "x_1 + x_2 + x_3 = 2\n",
    "$$\n",
    "\n",
    "This would be equivalent to solving the matrix equation\n",
    "\n",
    "$$\n",
    "\\mathbf{Mx}=\\mathbf{b}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{M} = \\begin{pmatrix} 3 & 5 & 2 \\\\ 2 & -3 & -3 \\\\ 1 & 1 & 1 \\end{pmatrix},\\quad \n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{pmatrix} 11 \\\\ -1 \\\\ 2 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "In this case of a system with just three equations, this is not difficult to do by hand, but often we'll need to be able to solve systems with an arbitrarily large number of equations and variables.\n",
    "\n",
    "More generally a system of $n$ equations of $n$ unknowns, such as\n",
    "\n",
    "$$\n",
    "m_{11} x_1 + m_{12} x_2 + \\cdots + m_{1n}x_n = b_1 \\\\\n",
    "m_{21} x_1 + m_{22} x_2 + \\cdots + m_{2n}x_n = b_2 \\\\\n",
    "\\qquad \\vdots \\qquad \\qquad \\qquad \\qquad \\vdots  \\\\\n",
    "m_{n1} x_1 + m_{n2} x_2 + \\cdots + m_{nn}x_n = b_n\n",
    "$$\n",
    "would be equivalent to solving the matrix equation\n",
    "\n",
    "$$\n",
    "\\mathbf{Mx}=\\mathbf{b}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{M} = \\begin{pmatrix}m_{11} & m_{12} & \\cdots & m_{1n} \\\\\n",
    "                            m_{21} & m_{22} & \\cdots & m_{2n} \\\\\n",
    "                            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                            m_{n1} & m_{n2} & \\cdots & m_{nn}\n",
    "         \\end{pmatrix},\\quad \n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "So we need a systematic way to do this.\n",
    "\n",
    "When might it not be possible to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "The usual approach to solving this kind of linear system is to use an algorithm called Gaussian Elimination. This can be done with the following procedure\n",
    "\n",
    "- Combine the solution vector with matrix of parameters to form the augmented matrix $\\mathbf{\\tilde{M}}$ as\n",
    "\n",
    "$$ \\mathbf{\\tilde{M}} = \\begin{pmatrix}m_{11} & m_{12} & \\cdots & m_{1n} & b_1\\\\\n",
    "                            m_{21} & m_{22} & \\cdots & m_{2n} & b_2\\\\\n",
    "                            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "                            m_{n1} & m_{n2} & \\cdots & m_{nn} & b_n \\end{pmatrix} $$\n",
    "                            \n",
    "- Perform row reduction on this matrix such that $\\mathbf{M}$ is transformed to an upper triangular form:\n",
    "\n",
    "$$ \\mathbf{\\tilde{M}'} = \\begin{pmatrix}m_{11}' & m_{12}' & \\cdots & m_{1n}' & b_1'\\\\\n",
    "                            0 & m_{22}' & \\cdots & m_{2n}' & b_2'\\\\\n",
    "                            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "                            0 & 0 & \\cdots & m_{nn}' & b_n' \\end{pmatrix} $$\n",
    "\n",
    "- Row reduction consists of the following operations, which are repeated as necessary.\n",
    "    1. Multiply any row by a constant.\n",
    "    2. Add any row to another row.\n",
    "    3. Swap the order of any two rows.\n",
    "\n",
    "Let's do this now for the simple example we gave above:\n",
    "\n",
    "We start with\n",
    "$$\n",
    "\\mathbf{\\tilde{M}}_0 = \\begin{pmatrix} 3 & 5 & 2 & 11\\\\ 2 & -3 & -3 & -1\\\\ 1 & 1 & 1 & 2\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We want to have zeros below some diagonal elements, so let's use the following procedure:\n",
    "- Given a diagonal element $m_{ii}$, to set element $m_{jk}$ to zero where $j>i$ (row) and $k<i$ (column), multiply row $i$ by $-m_{jk}/m_{ii}$ and add it to row $j$.\n",
    "\n",
    "Here we want to have zeros below the 3 in the first row. In the second row the value in that spot is 2, so we can multiply the first row by -2/3 and add it to the second to give a zero in that position. And similarly, we can multiply the first row by -1/3 and add it to the third to get a zero in that position in the third row:\n",
    "$$\n",
    "\\mathbf{\\tilde{M}}_1 = \\begin{pmatrix} 3 & 5 & 2 & 11\\\\ 0 & -19/3 & -13/3 & -25/3\\\\ 0 & -2/3 & 1/3 & -5/3\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now we want to get a zero in the second position in row 3 where we have -2/3, below the $-19/3$ in row 2. So as before we can multiply row 2 by $-2/3 \\times 3/19 = -2/19 $ and add row 3 to it to give \n",
    "$$\n",
    "\\mathbf{\\tilde{M}}_u = \\begin{pmatrix} 3 & 5 & 2 & 11\\\\ 0 & -19/3 & -13/3 & -25/3\\\\ 0 & 0 & 15/19 & -15/19\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now we can stop as the matrix is in upper diagonal form, so we have completed the row reduction step.\n",
    "\n",
    "- Back substitution is now used to generate the full solution.\n",
    "- If we write this as a set of equations again, it's clear we can solve directly these one at a time starting from the last one and working backwards as we will have one unknown each time.\n",
    "\n",
    "In the case of our example, we have the following set of equations:\n",
    "$$\n",
    "3 x_1 + 5 x_2 + 2 x_3 = 11\\\\\n",
    "-19/3 x_2 + -13/3 x_3 = -25/3\\\\\n",
    "15/19 x_3 = -15/19\n",
    "$$\n",
    "\n",
    "- Equation 3 tells us $x_3 = -1$.\n",
    "- Then equation 2 becomes $-19/3 x_2 = -25/3 + 13/3 = 38/3$, so $x_2 = 2$.\n",
    "- Then equation 1 becomes $3 x_1 = 11 - 10 + 2 = 3$, so $x_1 = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Factorization\n",
    "\n",
    "In the process of Gaussian Elimination, we have in effect multiplied the original matrix $\\mathbf{M}_0$ by another matrix, which we'll call $\\mathbf{N}$ for now, which transforms it to an upper triangular form $\\mathbf{U}$\n",
    "\n",
    "For example, in the first step with our example matrix:\n",
    "$$\\mathbf{M}_1 = \\mathbf{N}_0 \\mathbf{M}_0 =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ -2/3 & 1 & 0\\\\ -1/3 & 0 & 1\\end{pmatrix}\\cdot\n",
    "\\begin{pmatrix} 3 & 5 & 2\\\\ 2 & -3 & -3\\\\ 1 & 1 & 1\\end{pmatrix} = \n",
    "\\begin{pmatrix} 3 & 5 & 2\\\\ 0 & -19/3 & -13/3\\\\ 0 & -2/3 & 1/3\\end{pmatrix}$$\n",
    "\n",
    "The next step is:\n",
    "$$\\mathbf{M}_u = \\mathbf{N}_1 \\mathbf{M}_1 =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & -2/19 & 1 \\end{pmatrix}\\cdot\n",
    "\\begin{pmatrix} 3 & 5 & 2\\\\ 0 & -19/3 & -13/3\\\\ 0 & -2/3 & 1/3\\end{pmatrix} = \n",
    "\\begin{pmatrix} 3 & 5 & 2\\\\ 0 & -19/3 & -13/3\\\\ 0 & 0 & 15/19\\end{pmatrix}$$\n",
    "\n",
    "If we get the product of our $\\mathbf{N}$ matrices, we obtain the matrix that would transfrom $\\mathbf{M}$ to an upper triangular form in a single step.\n",
    "$$\\mathbf{U} = \\mathbf{N}_1 \\mathbf{N}_0 \\mathbf{M}_0$$\n",
    "So clearly we can write\n",
    "$$ \\mathbf{M}_0 = \\mathbf{N}_0^{-1} \\mathbf{N}_1^{-1} \\mathbf{U} = \\mathbf{LU}$$\n",
    "\n",
    "As the matrices $\\mathbf{N}_0$ and $\\mathbf{N}_1$ are fairly simple we can invert them by inspection.\n",
    "\n",
    "$\\mathbf{N}_1$ corresponds to multiplying the second row by $-19/2$ and adding it to the third, so the inverse would be to multiply the second row by $19/2$ and add it to the third.\n",
    "$$\\mathbf{N}_1^{-1} =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 2/19 & 1 \\end{pmatrix}$$\n",
    "\n",
    "$\\mathbf{N}_0$ corresponds to multiplying the first row by $-2/3$ and adding it to the second, and multiplying the first row by $-1/3$ and adding it to the third, so the inverse would instead multiply by $2/3$ and $1/3$ so that\n",
    "$$\\mathbf{N}_0^{-1} =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ 2/3 & 1 & 0\\\\ 1/3 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "And the product of these is also easier as we can simply combine the non-zero off-diagonal elements to give us\n",
    "$$\\mathbf{L} =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ 2/3 & 1 & 0\\\\ 1/3 & 2/19 & 1 \\end{pmatrix}$$\n",
    "which as we can see is lower diagonal.\n",
    "\n",
    "So altogether we have\n",
    "$$\\begin{pmatrix} 3 & 5 & 2\\\\ 2 & -3 & -3\\\\ 1 & 1 & 1\\end{pmatrix} =\n",
    "\\begin{pmatrix} 1 & 0 & 0\\\\ 2/3 & 1 & 0\\\\ 1/3 & 2/19 & 1\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix} 3 & 5 & 2\\\\ 0 & -19/3 & -13/3\\\\ 0 & 0 & 15/19\\end{pmatrix}$$\n",
    "\n",
    "The LU decomposition is most useful for when we want to solve $\\mathbf{Mx}=\\mathbf{b}$ for many different vectors $\\mathbf{b}$.\n",
    "\n",
    "In this situation we can write\n",
    "$$\\mathbf{Mx}=\\mathbf{LUx}=\\mathbf{L}(\\mathbf{Ux})=\\mathbf{b}$$\n",
    "\n",
    "From this we first set $\\mathbf{Ux}=\\mathbf{y}$ and solve $\\mathbf{Ly}=\\mathbf{b}$ by forward subtitution. Then use $\\mathbf{y}$ to solve for $\\mathbf{x}$ in $\\mathbf{Ux}=\\mathbf{y}$ by back substitution. If we already know $\\mathbf{L}$ and $\\mathbf{U}$ this takes far fewer operations than performing Gaussian elimination. This procedure, using LU decomposition to solve a linear system where we obtain $\\mathbf{L}$ with 1s along the diagonal, is sometimes referred to as Doolittle's method.\n",
    "\n",
    "Let's look at our first example again, but say our solution vector is now \n",
    "$$\\mathbf{b} = \\begin{pmatrix}2\\\\-1\\\\2 \\end{pmatrix}.$$\n",
    "We can first solve $\\mathbf{Ly}=\\mathbf{b}$:\n",
    "$$\\begin{pmatrix} 1 & 0 & 0\\\\ 2/3 & 1 & 0\\\\ 1/3 & 2/19 & 1 \\end{pmatrix} \\begin{pmatrix} y_1\\\\y_2\\\\y_3 \\end{pmatrix}\n",
    "=\\begin{pmatrix}2\\\\-1\\\\2 \\end{pmatrix} $$\n",
    "\n",
    "Which gives us $y_1 = 2$, $y_2 = -1 - 4/3 = -7/3$, $y_3 = 2 -2/3 +(2/19)(7/3)=30/19$.\n",
    "\n",
    "And now we solve $\\mathbf{Ux}=\\mathbf{y}$:\n",
    "$$\\begin{pmatrix}3 & 5 & 2\\\\ 0 & -19/3 & -13/3\\\\ 0 & 0 & 15/19 \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\x_3 \\end{pmatrix} =\\begin{pmatrix}2\\\\-7/3\\\\30/19 \\end{pmatrix} $$\n",
    "\n",
    "Which gives us:\n",
    "- $x_3 = 2$\n",
    "- $x_2 = (-7/3 + 26/3)(-3/19) = -1$\n",
    "- $x_1 = (2+5-4)/3=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Pivoting\n",
    "\n",
    "In our Gaussian elimination scheme as outlined above, the diagonal element $m_{ii}$ is called the *pivot*. We multiplied the elements of a row by a constant inversely proportional to the pivot. Clearly, there will be an issue if the pivot is zero; in this case we would need to swap the row with another.\n",
    "\n",
    "Less obviously, we will also have an issue if the pivot is small relative to other elements of the matrix. This would make the constant we multiply the row by very large, and will lead to us summing numbers of very different magnitudes, leading to a loss of precision in our code (see e.g. [here](https://gitlab.com/eamonn.murray/IntroToScientificComputing/tree/master/systems#real-numbers-floating-point-format).)\n",
    "\n",
    "To alleviate this, we use what is known as partial pivoting, where we swap rows such that we place the element with largest absolute value at the pivot.\n",
    "\n",
    "Say we want to calculate the LU decomposition of the following matrix $\\mathbf{M}$\n",
    "\n",
    "$$\\mathbf{M_0} = \\begin{pmatrix} 1 & -1 & 2 \\\\ -4 & 4 & 1 \\\\ -2 & 4 & 3 \\end{pmatrix}$$\n",
    "\n",
    "First we look at the first column, where the row 2 has the element with largest absolute value, so we swap row 2 and row 1.\n",
    "\n",
    "$$\\mathbf{M_0}' = \\begin{pmatrix} -4 & 4 & 1 \\\\ 1 & -1 & 2 \\\\ -2 & 4 & 3 \\end{pmatrix} = \\mathbf{P_{12}M_0}$$\n",
    "where $\\mathbf{P_{12}}$ is the permutation matrix that swaps row 1 and row 2:\n",
    "$$\\mathbf{P_{12}} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$.\n",
    "\n",
    "After the first step of Gaussian elimination we have now\n",
    "$$ \\mathbf{M_1} = \\begin{pmatrix} -4 & 4 & 1 \\\\ 0 & 0 & 9/4 \\\\ 0 & 2 & 5/2 \\end{pmatrix} = \\mathbf{N_0 M_0'}$$\n",
    "\n",
    "where\n",
    "$$ \\mathbf{N_0} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1/4 & 1 & 0 \\\\ -1/2 & 0 & 1 \\end{pmatrix} $$\n",
    "\n",
    "Now we want go the second diagonal element, so of the remaining rows, row 3 has the largest value element in the second column, and the value in the second row is zero in any case so we would not be able to continue without a permutation. So we can multiply with the matrix $\\mathbf{P_{23}}$ which swaps rows 2 and 3, and we'll have obtained an upper diagonal matrix.\n",
    "\n",
    "$$\\mathbf{P_{23}} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$$\n",
    "and\n",
    "$$\\mathbf{P_{23}M_1} = \\mathbf{U} = \\begin{pmatrix} -4 & 4 & 1 \\\\ 0 & 2 & 5/2 \\\\ 0 & 0 & 9/4 \\end{pmatrix}$$\n",
    "\n",
    "So combining the various operations we have\n",
    "$$ \\mathbf{P_{23}N_0P_{12}M_0} = \\mathbf{U}.$$\n",
    "\n",
    "Permutation matrices have the useful property that they are their own inverse, so we can write\n",
    "$$ \\mathbf{P_{23}N_0P_{23}^{-1}P_{23}P_{12}M_0} = (\\mathbf{P_{23}N_0P_{23})P_{23}P_{12}M_0} = \\mathbf{U}$$\n",
    "\n",
    "Pre-multiplying by a permutation vector exchanges rows, while post-multiplying exchanges columns, so\n",
    "$$\\mathbf{P_{23}N_0P_{23}} = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1/2 & 1 & 0 \\\\ 1/4 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "And we can write our factorization as\n",
    "$$ \\mathbf{P_{23}P_{12}M_0} = (\\mathbf{P_{23}N_0P_{23}})^{-1}\\mathbf{U} = \\mathbf{P M} = \\mathbf{LU}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python Implementation\n",
    "\n",
    "That all likely seems pretty complicated. Let's try writing Python/NumPy implementation of this, and hopefully you'll see it's not so bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivot_matrix(M):\n",
    "    \"\"\"Return the permuted matrix PM and pivot matrix P for M.\"\"\"\n",
    "    \n",
    "    # Start from the identity matrix\n",
    "    P = np.identity(len(M))\n",
    "    # PM will be calculated at the same time, starting from M\n",
    "    PM = M.copy()\n",
    "\n",
    "    # Rearrange the identity matrix such that the largest element of\n",
    "    # each column of M is placed on the diagonal of M\n",
    "    for i in range(len(M)):\n",
    "        max_row = i + np.argmax(np.absolute(PM[i:, i]))\n",
    "        if i != max_row:\n",
    "            # Swap the rows\n",
    "            P[i], P[max_row] = P[max_row], P[i].copy()\n",
    "            PM[i], PM[max_row] = PM[max_row], PM[i].copy()\n",
    "\n",
    "    return PM, P\n",
    "\n",
    "def lu_decomp(M):\n",
    "    \"\"\"Find the LU decomposition of square matrix M into PM = LU.\n",
    "    \n",
    "    The function returns P, L and U.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize L as the identity and U with zeros\n",
    "    L = np.identity(len(M))\n",
    "    U = np.zeros((len(M), len(M)))\n",
    "    \n",
    "    # Create the pivot matrix P\n",
    "    PM, P = pivot_matrix(M)\n",
    "\n",
    "    # Perform the LU decomposition on the permuted matrix\n",
    "    for j in range(len(M)):\n",
    "\n",
    "        # LaTeX: u_{ij} = m_{ij} - \\sum_{k=1}^{i-1} u_{kj} l_{ik}\n",
    "        for i in range(j+1):\n",
    "            s1 = sum(U[k, j] * L[i, k] for k in range(i))\n",
    "            U[i, j] = PM[i, j] - s1\n",
    "\n",
    "        # LaTeX: l_{ij} = \\frac{1}{u_{jj}} (m_{ij} - \\sum_{k=1}^{j-1} u_{kj} l_{ik} )\n",
    "        for i in range(j, len(M)):\n",
    "            s2 = sum(U[k, j] * L[i, k] for k in range(j))\n",
    "            L[i, j] = (PM[i, j] - s2) / U[j, j]\n",
    "\n",
    "    return (P, L, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the matrix used in the pivot example above\n",
    "M1 = np.array([[1, -1, 2], [-4, 4, 1], [-2, 4, 3]])\n",
    "\n",
    "P1, L1, U1 = lu_decomp(M1)\n",
    "print(\"P =\", P1, \"\\n\\nL =\", L1, \"\\n\\nU =\", U1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit lu_decomp(M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing this with SciPy\n",
    "\n",
    "The [linalg](https://docs.scipy.org/doc/scipy/reference/linalg.html) module in SciPy has many useful linear algebra routines. These call Fortran LAPACK library routines and are significantly faster than a native Python implementation.\n",
    "\n",
    "To perform an LU factorization, you can use the [`lu_factor`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_factor.html) function.\n",
    "\n",
    "The output of this can then be passed to the [`lu_solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html) function to use this factorization with a solution vector.\n",
    "\n",
    "Note there is also the [`lu`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html) function that returns the factorization in a more usable format, if you want to do something with the factorization yourself rather than simply use it with the `lu_solve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M1 = np.array([[1, -1, 2], [-4, 4, 1], [-2, 4, 3]])\n",
    "LU1, P1 = scipy.linalg.lu_factor(M1)\n",
    "print(\"LU =\", LU1, \"\\n\\n P indices = \", P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit scipy.linalg.lu_factor(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M1 = np.array([[3, 5, 2], [2, -3, -3], [1, 1, 1]])\n",
    "\n",
    "LU1, P1 = scipy.linalg.lu_factor(M1)\n",
    "\n",
    "b1 = np.array([11, -1, 2])\n",
    "print(scipy.linalg.lu_solve((LU1, P1), b1))\n",
    "\n",
    "b2 = np.array([2, -1, 2])\n",
    "print(scipy.linalg.lu_solve((LU1, P1), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the [`scipylinalg.solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html) function, and the [`numpy.linalg.solve`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html) function, which also use LU decomposition to solve a linear system. The SciPy version offers many more options that allow you to choose more optimal implementation when solving for e.g. symmetric or hermitian matrices.\n",
    "\n",
    "These functions do not, however, return the factorized matrices. This makes them more suitable for cases where you want to solve for a single solution vector $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.linalg.solve(M1,b1))\n",
    "print(scipy.linalg.solve(M1,b1))\n",
    "print()\n",
    "print(np.linalg.solve(M1,b2))\n",
    "print(scipy.linalg.solve(M1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit np.linalg.solve(M1, b1)\n",
    "%timeit scipy.linalg.solve(M1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small matrices, the overhead associated with processing the additional options in the SciPy implementation may make it somewhat slower than the NumPy equivalent. By default, both routines use the same LAPACK library routine to solve the system, so will take roughly the same size for larger matrices. We can confirm this by testing with large random matrices (which hopefully have a solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimrand = 2000\n",
    "Mrand = np.random.rand(dimrand, dimrand)\n",
    "brand = np.random.rand(dimrand)\n",
    "%timeit np.linalg.solve(Mrand, brand)\n",
    "%timeit scipy.linalg.solve(Mrand, brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "If you examine our Python implementation above, you'll see we have a triple nested loop. This means we likely have $O(n^3)$ scaling. Let's use `%timeit -o` to generate output we can save and try to see this ourselves. This generates a \"TimeitResult\" object which has few methods available for examining the output. We'll save the average of each run (these are in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solve_timing = []\n",
    "size = np.arange(10, 1000, 100)\n",
    "for s in size:\n",
    "    Mrand = np.random.rand(s, s)\n",
    "    brand = np.random.rand(s)\n",
    "    timer_out = %timeit -o scipy.linalg.solve(Mrand, brand)\n",
    "    solve_timing.append(timer_out.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set a scaling so the final points match up\n",
    "scale = solve_timing[-1] / size[-1]**3\n",
    "# Plot our timing results\n",
    "plt.plot(size, np.array(solve_timing), \"bo\")\n",
    "# And plot the scaling factor times size^3\n",
    "plt.plot(size, scale * size**3, 'r-')\n",
    "plt.xlabel(\"Matrix Size\")\n",
    "plt.ylabel(\"Time for solution (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banded Matrices\n",
    "\n",
    "One class of problem that comes up in many other methods is the banded matrix. \n",
    "Banded matrices only have non-zero elements along a diagonal band, consisting of the main diagonal and zero or more diagonals on either side. \n",
    "Let's denote the number of non-zero lower and upper diagonals as $k_u$ and $k_l$ respectively.\n",
    "- A matrix with $k_l = k_u = 0$ is a diagonal matrix.\n",
    "- An $n\\times n$ matrix with $k_l = 0$ and $k_u = n-1$ is an upper triangular matrix.\n",
    "- A matrix with $k_l = k_u = 1$ is called a tridiagonal matrix. This is a matrix where the only non-zero elements are along the main diagonal, and along the diagonals above and below it such as the following.\n",
    "\n",
    "$$ \\begin{pmatrix} \n",
    "m_{11} & m_{12} & 0 & 0 & \\cdots & 0 \\\\\n",
    "m_{21} & m_{22} & m_{23} & 0 & \\cdots & 0 \\\\\n",
    "0 & m_{32} & m_{33} & m_{34} & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & \\dots & m_{N-1, N-2} & m_{N-1, N-1} & m_{N-1, N} \\\\\n",
    "0 & \\cdots & \\dots & 0 & m_{N, N-1} & m_{N, N}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "For example, any problem with a 1D system where we approximate some piece of it as only interacting with its left and right neighbours can usually be cast as the solution of a tridiagonal matrix problem. (And by extension if we also included interactions with next-nearest neighbours it would be a banded matrix with $k_l = k_u = 2$.\n",
    "\n",
    "For these types of matrices there are better approaches we can use than doing e.g. a full LU factorisation using Gaussian elimination. If the matrix is diagonal, we can obtain the solutions directly. The usual method used for tridiagonal matrices is known as the Thomas algorithm, which is composed of a forward sweep which converts the matrix to upper triangular in a single pass, and is followed by backward substitution sweep to produce the result.\n",
    "\n",
    "When working with a banded matrix, it is wasteful to store that full matrix when most elements are 0. For example, if you're working with a tridiagonal matrix, it is better to create 3 arrays, 1 for the diagonal elements and 2 for the off-diagonal elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banded matrices in Python\n",
    "\n",
    "The [SciPy Linear algebra submodule](https://docs.scipy.org/doc/scipy/reference/linalg.html) contains the function [`solve_banded`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_banded.html) that can be used to efficiently find the solution of a banded matrix as described above. This is, as before, in fact an interface to a Lapack (fortran linear algebra library) function so will be quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(scipy.linalg.solve_banded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the help output, using this is a little more involved than in the previous cases where we could just pass our matrix and solution vector as arguments. This is because, as mentioned above, it's wasteful to store the full matrix if most elements are zero, so we need to ensure we're giving our matrix data in the correct format.\n",
    "\n",
    "Let's go through an example and hopefully this will become clear.\n",
    "\n",
    "Say we have a 10x10 tridiagonal matrix with $2$ along the diagonal ($m_{i,i} = 2$ for $0\\le i\\le 9$) and $-1$ along both subdiagonals ($m_{i, i+1} = m_{i+1, i} = - 1$ for $0 \\le i \\le 8$). And let's set the elements of solution vector $b$ to be all zero, except $b_4=b_5=-1$. Now we need to generate a set of appropriate arguments for the `solve_banded` function:\n",
    "- `(l, u)` will be `(1, 1)` since these correspond to our $k_l$ and $k_u$ above which are both 1 for a tridiagonal matrix.\n",
    "- `ab` stores our tridiagonal matrix in a compact form. For our 10x10 tridiagonal matrix this will be a 3x10 array, as follows, where a $*$ indicates an entry that won't be used.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    " * & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 \\\\\n",
    " 2 &  2 &  2 &  2 &  2 &  2 &  2 &  2 &  2 &  2 \\\\\n",
    "-1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 &  * \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- `b` stores our solution vector as before.\n",
    "\n",
    "Now let's write a write the code to solve this system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_packed = np.empty((3, 10)) # Initialize our 3x10 array\n",
    "m_packed[0] = -1 # We can just set the whole row to -1.\n",
    "m_packed[1] = 2 # Set row that stores the diagonals.\n",
    "m_packed[2] = -1 # Again we can just set the whole row to -1.\n",
    "# Let's output this to make sure it looks correct.\n",
    "print(\"m_packed = \\n\", m_packed)\n",
    "\n",
    "b = np.zeros(10) # And similarly for b, we'll intitialize it to zeros.\n",
    "# And now manually set the non-zero elements\n",
    "b[4] = -1\n",
    "b[5] = -1\n",
    "\n",
    "print(\"Solution =\", scipy.linalg.solve_banded((1,1), m_packed, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can compare this to the solution we get from the full matrix:\n",
    "\n",
    "# We first create arrays for the diagonal and subdiagonals\n",
    "m_diag = np.full(10, 2)\n",
    "m_subdiag = np.full(9, -1)\n",
    "\n",
    "# Now We'll use np.diag to convert these into full matrices and add them.\n",
    "# This makes constructing this kind of matrix a little easier.\n",
    "m_full = np.diag(m_diag, 0) + np.diag(m_subdiag, -1) + np.diag(m_subdiag, 1)\n",
    "# Let's output this to make sure it looks correct.\n",
    "print(\"m_full = \\n\", m_full)\n",
    "\n",
    "# We can use b as it is already.\n",
    "print(\"Solution =\", np.linalg.solve(m_full, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen that the full solver scales as $O(n^3)$. What about this banded matrix solver?\n",
    "\n",
    "Let's test the scaling for random tridiagonal matrices of increasing size in the same way we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bsolve_timing = []\n",
    "# Note we've scaled up our sizes by a factor of 10 compared to earlier\n",
    "bsize = np.arange(100, 10001, 1000)\n",
    "for s in bsize:\n",
    "    Mrand = np.random.rand(3, s)\n",
    "    brand = np.random.rand(s)\n",
    "    timer_out = %timeit -o scipy.linalg.solve_banded((1, 1), Mrand, brand)\n",
    "    bsolve_timing.append(timer_out.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(bsize, np.array(bsolve_timing), \"bo\")\n",
    "plt.xlabel(\"Matrix Size\")\n",
    "plt.ylabel(\"Time for solution (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that a tridiagonal matrix can be solved significantly faster. The tridiagonal matrix with nominally 10,000x10,000 elements (29,998 non-zero elements) is solved almost three orders of magnitude faster than a 1,000x1,000 regular matrix. And the tridiagonal matrix solver scales linearly, which the full matrix solution goes like $n^3$.\n",
    "\n",
    "If we were solving some matrix equation as part of a simulation of some system, we would be able to investigate significantly larger systems if we can use a banded matrix. In a system involving some set of interacting parts, this might involve truncating the interaction so that only neighbouring parts interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Timing for large matrix and large tridiagonal matrix with similar numbers of non-zero elements\n",
    "Mrand = np.random.rand(1000, 1000)\n",
    "brand = np.random.rand(1000)\n",
    "%timeit scipy.linalg.solve(Mrand, brand)\n",
    "\n",
    "Mrand = np.random.rand(3, 333334)\n",
    "brand = np.random.rand(333334)\n",
    "%timeit scipy.linalg.solve_banded((1, 1), Mrand, brand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
