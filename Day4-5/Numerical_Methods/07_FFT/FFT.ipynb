{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT and Ewald Summation\n",
    "\n",
    "The Fast Fourier Transform (FFT) is a method that is used frequently in the calculation of certain terms in the simulation of material, as well as being very important in signal and image processing. It is used to compute the discrete Fourier transform of a sequence or its inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Fourier Transform\n",
    "\n",
    "The discrete Fourier transform of a sequence of $N$ numbers $x_0$, $x_1$, $\\dots$, $x_{N-1}$ produces a corresponding sequence $X_0$, $X_1$, $\\dots$, $X_{N-1}$ given by\n",
    "\n",
    "$$ X_k = \\Sigma_{j=0}^{N-1} x_j \\mathrm{e}^{-i 2 \\pi j k/N}.$$\n",
    "\n",
    "The inverse transform is then given by\n",
    "\n",
    "$$ x_j = \\frac{1}{N}\\Sigma_{k=0}^{N-1} X_k \\mathrm{e}^{i 2 \\pi j k/N}.$$\n",
    "\n",
    "Note - this pair of transformations are often presented slightly differently. In particular how they are normalized and the signs of the exponents are chosen by convention. As long as both exponents have opposite sign, and the product of the normalization factors is $1/N$, any choice is fine. It's also common to see both transformations having a $1/\\sqrt{N}$ normalization factor for example.\n",
    "\n",
    "For example, let's say $x_j$ represents the sampled position of an atom every timestep $\\tau$. We could use the discrete Fourier transform to find the frequencies with which the atom is vibrating. Each point of the transform $X_k$ will correspond to a frequency $\\frac{k}{\\tau N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's start with a function that generates some oscillating data using a sin\n",
    "def vib_data1(num_points, freq, phase, timestep):\n",
    "    '''Generate N points from a given sine wave'''\n",
    "    # Initialize an array of the correct size, and assign the correct\n",
    "    # value to each element in a loop.\n",
    "    samples = np.empty(num_points)\n",
    "    for it in range(num_points):\n",
    "        samples[it] = np.sin(2 * np.pi * freq * timestep * it + phase)\n",
    "    return samples\n",
    "\n",
    "# Here's another small diversion looking at how we might make things fast with numpy.\n",
    "\n",
    "def vib_data2(num_points, freq, phase, timestep):\n",
    "    '''Generate N points from a given sine wave'''\n",
    "    # Use a list comprehension to generate a list of all the arguments we want to pass\n",
    "    # to the sin function to generate our sampled data. Then use the numpy sin function\n",
    "    # on this whole array, as it will be applied element-wise.\n",
    "    sin_args = np.array([(2 * np.pi * freq * it * timestep + phase) for it in range(num_points)])\n",
    "    return np.sin(sin_args)\n",
    "\n",
    "# We could also do this in a single command. Which do you think is faster, and why?\n",
    "def vib_data3(num_points, freq, phase, timestep):\n",
    "    '''Generate N points from a given sine wave'''\n",
    "    return np.array([np.sin(2 * np.pi * freq * it * timestep + phase) for it in range(num_points)])\n",
    "\n",
    "# Another optimization we could make is to take the constant terms from sin_args\n",
    "# and multiply them outside the list comprehension. The same can be done with the phase. Allowing us\n",
    "# to switch to a full set of vector operations.\n",
    "def vib_data4(num_points, freq, phase, timestep):\n",
    "    '''Generate N points from a given sine wave'''\n",
    "    arg_factor = 2 * np.pi * freq * timestep\n",
    "    t_indices = np.arange(num_points) # This makes an array with numbers 0 to num_points-1\n",
    "    sin_args = t_indices * arg_factor + phase\n",
    "    return np.sin(sin_args)\n",
    "\n",
    "# All these routines output the exact same thing for the same inputs.\n",
    "# They all differ slightly in their implementation though.\n",
    "# You can compare the speed of them using the %timeit ipython magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's define a function for the forward transformation\n",
    "# We're going to do this explicitly to start with so you can see it scales as O(N^2)\n",
    "def forward_dft(xl):\n",
    "    '''Forward discrete Fourier transform'''\n",
    "    N = len(xl)\n",
    "    Xk = np.zeros(N, dtype=complex)\n",
    "    expfactor = -2j * np.pi / N\n",
    "    # Let's do this explicitly to start with\n",
    "    for k in range(N):\n",
    "        for l in range(N):\n",
    "            Xk[k] = Xk[k] + xl[l] * np.exp(expfactor * k * l)\n",
    "    return Xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize and plot our sampled data\n",
    "n = 20; f= 0.1; p = np.pi * 0; t = 1;\n",
    "samples = vib_data1(num_points=n, freq=f, phase=p, timestep=t)\n",
    "plt.plot(samples)\n",
    "plt.show()\n",
    "\n",
    "# And let's try our transform with the samples we generated\n",
    "ft_samples = forward_dft(samples)\n",
    "#print(ft_samples)\n",
    "freqs = np.arange(len(samples)) / (t * len(samples))\n",
    "# The output is complex so we plot the real and imaginary parts.\n",
    "plt.plot(freqs, np.real(ft_samples), label=\"Real\")\n",
    "plt.plot(freqs, np.imag(ft_samples), label=\"Imaginary\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have generated samples using a sine wave with frequency 0.1, and we get peaks in the imaginary part of the transform at 0.1 and 0.9.\n",
    "\n",
    "Let's try the following, and see how it affects the transform each time\n",
    "- change the input frequency to 0.3\n",
    "    - You should now see peaks in the transform at 0.3 and 0.7\n",
    "- change the phase to $\\pi/2$ to make our sampled wave a cosine\n",
    "    - This leads to a change of phase in the transformed signal also\n",
    "- change the frequency to 0.11\n",
    "    - The structure now looks quite different. This will happen for any frequency that's not a multiple of $\\frac{1}{\\tau N}$. For this reason it can be useful to instead look at the power spectrum, by calculating the absolute square at each transformed point. Try doing this now. Note - you can square a numpy complex array, but you'll need to take the real part to plot it as the output is still complex.\n",
    "    \n",
    "You may have noticed our transform peaks are at $f$ and $1-f$. But why are we getting two peaks rather than 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aliasing\n",
    "\n",
    "In the first case, we got peaks in our transform at $f=0.1$ and $f=0.9$. Let's try generating samples at both of these frequencies and comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_a = vib_data1(num_points=20, freq=0.1, phase=0.0, timestep=1.0)\n",
    "samples_b = vib_data1(num_points=20, freq=0.9, phase=0.0, timestep=1.0)\n",
    "\n",
    "plt.plot(samples_a, 'bo')\n",
    "plt.plot(samples_b, 'r+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled data looks like it has the same frequency for both cases, but opposite phase! How did this happen?\n",
    "\n",
    "This occurs due to a phenomenom called aliasing. If we decrease the sampling period (timestep), and increase the total number of samples correspondingly, we should be able to resolve these two sets of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_a = vib_data1(num_points=2000, freq=0.1, phase=0.0, timestep=0.01)\n",
    "samples_b = vib_data1(num_points=2000, freq=0.9, phase=0.0, timestep=0.01)\n",
    "\n",
    "times = np.arange(20, step=0.01) # Since our timestep isn't 1 we need to set the x-axis values.\n",
    "plt.plot(times, samples_a, 'bo')\n",
    "plt.plot(times, samples_b, 'r+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's clear these two curves are different, but for the sampling interval we had, they both return the same points (but with opposite phase).\n",
    "\n",
    "Waves sampled in steps of $\\tau$ thus have a limit as to how high a frequency we can resolve. This is given by\n",
    "$$ f_{Ny} = \\frac{1}{2\\tau} $$\n",
    "which is known as the Nyquist frequency.\n",
    "\n",
    "So with $\\tau=1$ we should truncate our transformed output at a frequency of 0.5. While it may seem we are throwing data away by doing this, our transform of N real samples generated N complex numbers. This means that the information in our transformed output must contain duplication as the amount of information can't have doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFT\n",
    "\n",
    "Look at the implementation of the discrete Fourier transform above. You can see that as we have a double nested loop, each with $N$ elements, the implementation will scale with O($N^2$).\n",
    "\n",
    "The Fast Fourier Transform (FFT) algorithm is based on a clever reorganisation of the nested sum that improves the scaling to O($N\\log N$). The method originally relied on $N$ being a power of 2 so that the summation could be repeatedly broken down into smaller parts. It has been steadily generalized since then, so that even prime values of $N$ can achieve O($N\\log N$) scaling.\n",
    "\n",
    "The FFT will produce exactly the same output as the naive implementation above, but allows significantly larger data sets to be used.\n",
    "\n",
    "Modern implementations are fairly complex, but thankfully, libraries to perform the FFT are ubiqitous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "NumPy has a module dedicated to the discrete Fourier transform: [numpy.fft](https://docs.scipy.org/doc/numpy/reference/routines.fft.html). This has range of functions available depending on the dimensionality and other characteristics of the data you are analysing.\n",
    "\n",
    "Let's use one with our samples as before. We'll use the [rfft()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.rfft.html) function which is written for real input, as all our sampled data is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 20; f= 0.1; p = np.pi * 0; t = 1;\n",
    "samples = vib_data1(num_points=n, freq=f, phase=p, timestep=t)\n",
    "plt.plot(samples)\n",
    "plt.show()\n",
    "\n",
    "# We'll use the rfft function which is for real input\n",
    "ft_samples = np.fft.rfft(samples)\n",
    "# Note: the number of points returned by this function is\n",
    "# (n/2)+1 for an even number of samples, and (n+1)/2 for an odd number\n",
    "# due to the aliasing issue discussed earlier.\n",
    "\n",
    "# To get the associated frequencies, this helper function can be used.\n",
    "freqs = np.fft.rfftfreq(len(samples), t)\n",
    "\n",
    "# The output is complex so we plot the real and imaginary parts.\n",
    "plt.plot(freqs, np.real(ft_samples), label=\"Real\")\n",
    "plt.plot(freqs, np.imag(ft_samples), label=\"Imaginary\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# And lets calculate the inverse of this to confirm we get our sampled data back\n",
    "ift_samples = np.fft.irfft(ft_samples)\n",
    "plt.plot(ift_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy\n",
    "\n",
    "There are also more detailed implementations available in the SciPy module [fftpack](https://docs.scipy.org/doc/scipy/reference/fftpack.html). This module offers a wider range of functions than the equivalent NumPy module, and can be a little better optimized also.\n",
    "\n",
    "Where equivalent NumPy and SciPy functions are available, it's generally best to chose the SciPy version as these are the versions where development is focused. We could use the [rfft](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.rfft.html) SciPy function in exactly the same way as the NumPy version used above, although the conventions for the output are a little different, with the SciPy version returning a real array containing the complex parts of the transform (which is honestly pretty awkward to work with - there has been some discussion about changing this, or making new scipy functions with numpy style output, so be sure to check the doc pages if you're referring to this notebook from the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.fftpack\n",
    "ft_samples = scipy.fftpack.rfft(samples)\n",
    "freqs = scipy.fftpack.rfftfreq(len(ft_samples), t)\n",
    "\n",
    "# The output is in a single array containing both the real and imaginary components\n",
    "plt.plot(freqs, np.real(ft_samples), label=\"Real & Imaginary\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# And lets calculate the inverse of this to confirm we get our sampled data back\n",
    "ift_samples = scipy.fftpack.irfft(ft_samples)\n",
    "plt.plot(ift_samples)\n",
    "plt.show()\n",
    "\n",
    "# Compare the speed of both functions\n",
    "%timeit np.fft.rfft(samples)\n",
    "%timeit scipy.fftpack.rfft(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutidimensional Discrete Fourier Transform\n",
    "\n",
    "One of the main uses of the discrete Fourier transform you will come across is in the transformation of quantities such as electronic charge density from real space to reciprocal (wavevector) space in calculations where periodic boundary conditions are used. This is a three-dimensional problem rather than the one dimensional problem we've looked at so far.\n",
    "\n",
    "The expression for the three dimensional discrete Fourier transform is given by nesting three one-dimensional transformations as follows:\n",
    "$$ X_{k_1, k_2, k_3} = \\Sigma_{j_1=0}^{N_1-1} \\left[\\mathrm{e}^{-i 2 \\pi j_1 k_1/N_1} \\Sigma_{j_2=0}^{N_2-1} \\left(\\mathrm{e}^{-i 2 \\pi j_2 k_2/N_2} \\Sigma_{j_3=0}^{N_3-1} x_j \\mathrm{e}^{-i 2 \\pi j_3 k_3/N_3}\\right)\\right].$$\n",
    "\n",
    "This is more usually written in terms of vectors $\\mathbf{k}$ and $\\mathbf{j}$ as\n",
    "$$ X_\\mathbf k = \\Sigma_{\\mathbf j=\\mathbf 0}^{\\mathbf N-1} x_\\mathbf j \\mathrm{e}^{-i 2 \\pi \\mathbf j\\cdot (\\mathbf k/\\mathbf N)},$$\n",
    "\n",
    "where $\\mathbf N -1 = (N_1-1, N_2-1, N_3-1)$ and $\\mathbf k /\\mathbf N = (k_1/N_1, k_2/N_2, k_3/N_3)$.\n",
    "\n",
    "There is both a [numpy.fft.fftn](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftn.html) and a [scipy.fftpack.fftn](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.fftn.html) function which can be used to calculate n-dimensional FFT. Note there are also two dimensional versions if you are working with a 2D model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses in Materials Simulations\n",
    "\n",
    "### The Hartree term\n",
    "\n",
    "If we have some distribution of electrons $n(r)$ in a periodic system, it's common to calculate the their electrostatic interaction with each other as a single term contributing to the total energy of the system (along with several other terms depending on what method is being used). This is usually referred to as the Hartree term and is given by:\n",
    "$$ E_\\mathrm H = \\frac{1}{2} \\int \\int \\frac{n(\\mathbf r) n(\\mathbf r')}{|\\mathbf r-\\mathbf r'|}\n",
    "\\mathbf d\\mathbf r \\mathbf d\\mathbf r' $$\n",
    "\n",
    "As you might imagine, this term when considered in isolation, diverges since it only considers the negative electrons and  we have a periodic (infinitely repeating) system. You can see this if you consider a one dimensional system consisting of a chain of equal charges, $q$, per cell of dimension $a$, and each charge at position $x$ within its cell. The energy of these charges interacting with each other if we consider $N$ neighbouring cells is given by\n",
    "$$ E_e = \\frac{1}{2} \\Sigma_{i=1}^N \\Sigma_{j=1,j\\ne i}^N \\frac{q^2}{|x_i - x_j|}\n",
    "= N\\left(\\frac{q^2}{2a}\\right)\\Sigma_{i=1}^N \\frac{1}{i} $$\n",
    "which diverges as $N$ is increased.\n",
    "\n",
    "Fortunately, the diverging term is cancelled by similar terms in the ion-ion and electron-ion terms when all three are taken together. We can then calculate the Hartree term efficiently with an FFT as\n",
    "$$ E_\\mathrm H = \\frac{1}{2} \\int \\int \\frac{n(\\mathbf r) n(\\mathbf r')}{|\\mathbf r-\\mathbf r'|}\n",
    "\\mathrm d\\mathbf r \\mathrm d\\mathbf r'\n",
    "= \\frac{2\\pi}{V}\\Sigma_{\\mathbf G\\ne 0} \\frac{|\\tilde n(\\mathbf G)|^2}{G^2}$$\n",
    "where $V$ is the cell volume and $\\tilde n(\\mathbf G)$ is reciprocal space charge distribution, given by the FFT of the real space charge distribution $n(\\mathbf r)$. The $G=0$ term is infinite and corresponds to the divergent part of the Hartree term that would be cancelled by other contributions to the energy as long as the system is charge neutral overall, so we can omit it here. So we can replace a fairly intensive double integral over real space, with a sum over FFT coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewald Summation\n",
    "\n",
    "Another important application of the FFT in the simulation of materials is in Ewald summation. The problem is quite similar to that outlined in the case of the Hartree term above. This time we have some grid of point charges. This would be the case for the nuclei in any calculation where we use the Born-Oppenheimer approximation and treat the nuclei as fixed points and solve for the electrons - we still need to add in a term that represent the interaction of these positive nuclei with each other at the end. This electrostatic contribution to the total energy of a set of ions with charge $Z_i$ and position $\\mathbf R_i$ will be given by\n",
    "$$ E_I = \\frac{1}{2} \\Sigma_{i, j, i\\ne j} \\frac{Z_i Z_j}{|\\mathbf R_i -\\mathbf R_j|} $$\n",
    "\n",
    "We could think of our set of point charges as represented by delta function distributions and write this in a similar way as we the Hartree energy term. However, the Fourier transform of a delta function is completely delocalized, so trying to sum over our Fourier coefficients will also give us convergence problems.\n",
    "\n",
    "The way to get around this is to break up this slowly converging sum into two parts, one of which is done in real space, with the other in reciprocal space, both of which are well behaved and converge rapidly.\n",
    "- In real space we have our original lattice of charges, and around each we put some smooth and smeared out distribution of opposite charge, such as a Gaussian (typically Error functions are used rather than Gaussians in practice). The energy of this charge distribution will converge rapidly as at long range the oppositely charged Gaussian distributions will exactly cancel the delta functions due to the point charges. So this is short ranged in real space.\n",
    "- Now we have another distribution of Gaussians that we need to add to counteract the set we have added. These are spread out in real space, which means they will be localized in reciprocal space. So we can handle these in the same way as the Hartree term described earlier.\n",
    "\n",
    "In this way, the FFT can be used to calculate this term very rapidly.\n",
    "\n",
    "In practice, whenever you need to integrate over something that's very delocalized in real space, consider whether it may be beneficial to calculate the Fourier transform and perform the integral in reciprocal space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
