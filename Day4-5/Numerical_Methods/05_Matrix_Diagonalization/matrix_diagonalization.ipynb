{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Diagonalization\n",
    "\n",
    "Matrix diagonalization is another type of matrix factorization. We already saw the LU factorization when we looked at how to solve linear systems of equations.\n",
    "\n",
    "Say we have a square matrix $A$, to diagonalize it we want to factorize it as\n",
    "$$ A = PDP^{-1} $$\n",
    "where $D$ is a diagonal matrix (only the diagonal elements are non-zero). Here the matrix $P$ is composed of the eigenvectors ($\\vec v_i$) of $A$, and the elements of $D$ are the eigenvalues ($\\lambda_i$) of $A$.\n",
    "\n",
    "So with the matrix $P$ composed of a set of column vectors $\\vec v_i$, we can also write the above expression as\n",
    "$$ A\\vec v_i = \\lambda_i\\vec v_i. $$\n",
    "\n",
    "You might recognize this form as that also taken by the time-independent Schr√∂dinger equation: $\\mathrm{\\hat H}\\Psi = E\\Psi$. So matrix diagonalization arises very frequently in quantum mechanical models.\n",
    "\n",
    "We can rewrite Eq. (2) in a form like the linear systems we looked at previously\n",
    "$$ (A-\\lambda_i I)\\vec v_i = \\vec 0. $$\n",
    "We want to find solutions of this where $\\vec v$ is non-zero, which implies that the determinant of the matrix $(A-\\lambda I)$ must be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristic polynomial and eigenvalues\n",
    "\n",
    "This bring us to the characteristic polynomial of a matrix. This is defined by the expression above:\n",
    "$$\n",
    "\\textrm{det}(A-\\lambda I) = 0.\n",
    "$$\n",
    "\n",
    "For example, if we have the matrix\n",
    "$$ A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}. $$\n",
    "This gives us the following expression for the characteristic polynomial:\n",
    "$$ \\textrm{det}\\begin{pmatrix} 2 - \\lambda & 1 \\\\ 1 & 2 - \\lambda \\end{pmatrix} = 0. $$\n",
    "We can expand this to\n",
    "$$ (2-\\lambda)^2 - 1 = 3 - 4 \\lambda + \\lambda^2 = 0. $$\n",
    "\n",
    "If we solve this equation for $\\lambda$ we'll obtain the eigenvalues of $A$. We can factor this as $(\\lambda-1)(\\lambda-3)$, meaning that our two eigenvalues are $\\lambda=1$ and $\\lambda=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors\n",
    "\n",
    "Once we have obtained the eigenvalues, we can obtain the eigenvectors corresponding to each one by solving the matrix equation. For example, to find the eigenvector corresponding to the eigenvalue $\\lambda = 1$ above, we need to solve\n",
    "$$ \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}. $$\n",
    "We can see that any $\\vec v$ where $v_1 = -v_2$ would satisfy this expression. Alternatively we could say the eigenvector is any (non-zero) scalar multiple of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n",
    "\n",
    "For the eigenvalue $\\lambda=3$ we need to solve\n",
    "$$ \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}. $$\n",
    "In this case, any $\\vec v$ where $v_1 = v_2$ would satisfy this expression. So we can say the eigenvector is any scalar multiple of $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n",
    "\n",
    "Note that since the eigenvector corresponding to a given eigenvalue can always be multiplied by a non-zero scalar and still be a valid eigenvector, they are often normalized such that $\\left|\\vec v\\right| = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical approaches for calculating eigenvalues\n",
    "\n",
    "Finding the eigenvalues from the characteristic polynomial in the example above was straightforward as this was just a $2\\times2$ matrix. The more general problem of finding the eigenvalues of an $n\\times n$ matrix is much more difficult, and is typically solved numerically for any moderately sized matrix. Many methods are available, each with their own advantages and disadvantages, in terms of scaling, stability, suitability for different types of matrices (e.g. sparse, banded, symmetric, real/complex, etc), and what information we want to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Iteration\n",
    "\n",
    "Power iteration can be used when we want to find the eigenvalue with largest absolute value in a matrix, and its associated eigenvector. These are termed the dominant eigenvalue and dominant eigenvector. This is one of the easier algorithms to follow. It consists of an iterative procedure to find the dominant eigenvalue and eigenvector of a matrix $A$:\n",
    "- Pick an initial guess for the eigenvector $\\vec v_0$. It must have a non-zero component along the dominant eigenvector.\n",
    "- Calculate $v_1=A\\vec v_0$.\n",
    "- Renormalize $\\vec v_1$.\n",
    "- $\\vec v_1$ is now a better approximation to the dominant eigenvector.\n",
    "- Repeat this process, with $\\vec v_{i+1} = A\\vec v_i/|A\\vec v_i|$ for each iteration, until a specified number of iterations have been completed or some other convergence criteria has been fulfilled.\n",
    "- The eigenvalue associated with an eigenvector can be calculated using the Rayleigh quotient:\n",
    "$$ \\lambda_i = \\frac{A \\vec v \\cdot \\vec v}{\\vec v \\cdot \\vec v}. $$\n",
    "    - If $\\vec v$ is normalized this is simplified to $\\lambda = A\\vec v \\cdot \\vec v$.\n",
    "\n",
    "Let's write a Python function to implement this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(a, eig_conv=0.0001, max_iter=100):\n",
    "    '''\n",
    "    Find the dominant eigevalue and eigenvector of a square matrix.\n",
    "    \n",
    "    Use the power iteration algorithm to find the dominant\n",
    "    eigenvalue and eigenvector of a matrix. The eigenvalue\n",
    "    is evaluted on each step using the Rayleigh quotient and\n",
    "    convergence is tested against its relative change.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : (N, N) array_like\n",
    "        The matrix we want to analyse, in numpy format.\n",
    "    eig_conv : (float, optional)\n",
    "        The process is converged when the relative change in the eigenvalue\n",
    "        is less than this number.\n",
    "    max_iter : (int, optional)\n",
    "        The maximum number of iterations before the function exits.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    steps : int\n",
    "        The number of steps to convergence. This would not be returned in a\n",
    "        proper implementation, but it's interesting to see here how many steps\n",
    "        it actually takes to converge.\n",
    "    eigval : float\n",
    "        The dominant eigenvalue.\n",
    "    eigvec : array\n",
    "        The dominant eigenvector.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    LinAlgError\n",
    "        If the max number of iterations are reached before convergence.\n",
    "                  \n",
    "    '''\n",
    "    # We can start from a random guess for the eigenvector.\n",
    "    eigvec = np.random.rand(a.shape[0])\n",
    "    eigvec = eigvec / np.linalg.norm(eigvec)\n",
    "    eigval = np.dot(np.dot(a, eigvec), eigvec)\n",
    "    \n",
    "    for steps in range(max_iter):\n",
    "        # Calculate our new eigvec from our previous approximation.\n",
    "        eigvec = np.dot(a, eigvec)\n",
    "        # Renormalize it.\n",
    "        eigvec = eigvec / np.linalg.norm(eigvec)\n",
    "        \n",
    "        # Calculate the new eigenvalue.\n",
    "        eigval_new = np.dot(np.dot(a, eigvec), eigvec)\n",
    "        # Test the relative change with respect to the previous approximation.\n",
    "        if abs((eigval_new - eigval) / eigval) < eig_conv:\n",
    "            return steps, eigval, eigvec\n",
    "        # Update our approximation for the eigenvalue.\n",
    "        eigval = eigval_new\n",
    "        \n",
    "    # If we finish the for loop without converging, raise an error.\n",
    "    raise np.linalg.LinAlgError('power_iteration failed to converge to requested tolerance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2.999999999817665, array([0.70710453, 0.70710903]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_iteration(np.array([[2, 1], [1, 2]]), eig_conv=1e-10)\n",
    "# For our simple system we had previously, you can converge very accurately in a fairly small number of steps.\n",
    "# Since we have a random starting point, the number of steps to convergence can vary a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " 49.88562363174823,\n",
       " array([0.0973016 , 0.10382377, 0.09595256, 0.08918906, 0.10418911,\n",
       "        0.09748208, 0.11374433, 0.10291882, 0.09263224, 0.09643225,\n",
       "        0.10466302, 0.1081008 , 0.10411001, 0.08958756, 0.09680659,\n",
       "        0.1018447 , 0.11507118, 0.09434534, 0.09704463, 0.09784891,\n",
       "        0.08997881, 0.1042847 , 0.10477967, 0.10556942, 0.09083886,\n",
       "        0.09852254, 0.11171396, 0.09995302, 0.10236582, 0.10615271,\n",
       "        0.10574805, 0.09560456, 0.09839899, 0.10572321, 0.10447481,\n",
       "        0.09834469, 0.09555497, 0.10613201, 0.09177173, 0.10314074,\n",
       "        0.09750844, 0.10919614, 0.10193561, 0.11850685, 0.10245568,\n",
       "        0.09894077, 0.09294213, 0.09409628, 0.11311262, 0.09681903,\n",
       "        0.10661928, 0.09398816, 0.09514876, 0.10252927, 0.09110701,\n",
       "        0.1125416 , 0.10411962, 0.10069137, 0.10482574, 0.0952345 ,\n",
       "        0.10659132, 0.0927492 , 0.09201186, 0.10606817, 0.08142181,\n",
       "        0.09884021, 0.1011338 , 0.1014121 , 0.09557973, 0.09581024,\n",
       "        0.10430032, 0.09798918, 0.10711439, 0.10514102, 0.09428093,\n",
       "        0.10141941, 0.09829282, 0.09652805, 0.09261382, 0.08945392,\n",
       "        0.10367815, 0.09118947, 0.09759464, 0.10661131, 0.10063146,\n",
       "        0.10631493, 0.0997441 , 0.09779386, 0.09481669, 0.10026369,\n",
       "        0.10438136, 0.09052635, 0.09238832, 0.11095704, 0.09948158,\n",
       "        0.09048681, 0.0945476 , 0.09954915, 0.08911207, 0.09464919]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see what we get for a large random matrix\n",
    "rand_mat = np.random.rand(100, 100)\n",
    "power_iteration(rand_mat, eig_conv=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try playing around with this for random matrices of different sizes. Do you notice anything about the leading eigenvalue each time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, convergence is most slow when there is a small separation between the dominant eigenvalue and the next most dominant. This means convergence will also be slow if the dominant eigenvalue is degenerate, in which case you'll converge to the eigenvector closest your initial guess, and this may not be consistent if you start from a random guess as above.\n",
    "\n",
    "This can be quite an efficient method for very large sparse matrices, as no explicit decomposition is needed. This is used, for example, by search engines to calculate the page rank of a website, or to show recommended users on social websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 1.]),\n",
       " array([[ 1.00000000e+00, -1.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  4.44089210e-16, -9.48683298e-01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  3.16227766e-01]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what happens for a matrix with a degenerate dominant eigenvalue\n",
    "m = np.array([[2, 1, 3], [0, 2, 3], [0, 0, 1]])\n",
    "np.linalg.eig(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316,\n",
       " 2.0063266622373774,\n",
       " array([9.99980112e-01, 6.30683824e-03, 5.75864762e-99]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_iteration(m, eig_conv=1e-5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Decomposition\n",
    "\n",
    "The approach most commonly used in many numerical libraries is based on QR decomposition. The QR decomposition, factorizes a square matrix $A$ as\n",
    "$$ A = QR, $$\n",
    "where $Q$ is an orthogonal matrix, meaning it's columns are orthogonal unit vectors, and $R$ is an upper triangular matrix (as was $U$ in the LU decomposition we covered previously).\n",
    "\n",
    "The QR algorithm proceeds as follows:\n",
    "- Starting from the matrix we wish to diagonalize, $A_0$, we calculate the QR decomposition, to find $Q_0$ and $R_0$.\n",
    "- We then calculate $A_1$ as $R_0 Q_0$.\n",
    "- The QR decomposition of $A_1$ is then found.\n",
    "- This process is iterated with $A_{i+1} = R_i Q_i$\n",
    "- $A_i$ will converge to an upper triangular matrix for most matrices. The diagonal elements of an upper triangular matrix equal its eigenvalues.\n",
    "- This works as $A_{i+1} = R_i Q_i = Q_i^{-1}Q_i R_i Q_i = Q_i^{-1}A_i Q_i$, meaning that $A_i$ and $A_{i+1}$ are similar (have the same eigenvalues) since $Q$ is an orthogonal matrix.\n",
    "- This method functions in a similar way to the power iteration method, but calculates all eigenvalues and eigenvectors simultaneously, by multiplying by a full matrix rather than a single vector.\n",
    "- In practice, some additional steps are done to optimize this process, such as using so-called [Householder transformations](https://en.wikipedia.org/wiki/Householder_transformation) to convert $A$ to an upper Hessenberg form (almost upper triangular), so implementations can be somewhat involved.\n",
    "- Also if you want to obtain the eigenvectors as well as eigenvalues, the product of the $Q_i$ of each step needs to be accumulated, effectively giving you what is known as the _Schur decomposition_ ($A=QUQ^{-1}$, where $Q$ is unitary and $U$ is upper triangular), which can then be used to find the eigenvectors efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide and Conquer\n",
    "\n",
    "We'll just go through a general overview of this to show you how different it is to the QR approach we discussed earlier. It's interesting how for certain types of matrices a very different approach can be more optimal then for a general matrix. In this case, the approach works out to be slightly more optimal for symmetric and hermitian matrices. It proceeds as follows:\n",
    "- The matrix $M$, is first converted to an tridiagonal form $T$, using [Householder transformations](https://en.wikipedia.org/wiki/Householder_transformation).\n",
    "- The tridiagonal matrix is then converted to a a block diagonal matrix plus a rank 1 correction matrix. This is the divide step, and produces the matrices from the upper and lower blocks $T_1$ and $T_2$, and the correction matrix $\\beta$.\n",
    "- Then there is conquer step that involves generating the diagonalization of the original matrix from the diagonalization of these submatrices.\n",
    "\n",
    "There's more detail on this algorithm at the [wiki page](https://en.wikipedia.org/wiki/Divide-and-conquer_eigenvalue_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonalizing Matrices in Python\n",
    "\n",
    "There are a range of functions available, both from Numpy and SciPy to find the eigenvalues and eigenvectors of matrices. These are all interfaces to various LAPACK (a fast Fortran linear algebra library) functions, with different functions offering more optimal approaches for certain matrix types (recall banded matrices had a better scaling approach when solving linear systems). Functions to return selveral of the factorizations we've discussed are also available.\n",
    "\n",
    "One thing to keep in mind is that the eigenvalues and eigenvectors of a real matrix can be complex, so these will usually return arrays with complex values even if your input is real, except in the case of a real symmetric or Hermitian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy functions\n",
    "\n",
    "- [numpy.linalg.eigvals](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvals.html) will compute the eigenvalues of a square array\n",
    "- [numpy.linalg.eig](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) will compute the eigenvalues and eigenvectors of a square array.\n",
    "    - Both `eig()` and `eigvals()` use calls to the [geev](https://software.intel.com/en-us/mkl-developer-reference-fortran-geev) LAPACK routines for general square matrices, which uses the QR algorithm.\n",
    "    - There are also versions of these functions for Hermitian or real symmetric matrices: [numpy.linalg.eigvalsh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvalsh.html) and [numpy.linalg.eigh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html), which use the [syevd](https://software.intel.com/en-us/mkl-developer-reference-fortran-syevd) (real symmetric), and [heevd](https://software.intel.com/en-us/mkl-developer-reference-fortran-heevd) (hermitian) LAPACK routines. Both of these use a [divide and conquer algorithm](https://en.wikipedia.org/wiki/Divide-and-conquer_eigenvalue_algorithm).\n",
    "- [numpy.linalg.qr](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.qr.html) is also available for calculating the QR decomposition directly. This is also an interface to LAPACK routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPy functions\n",
    "\n",
    "There are many similarly available SciPy functions, which are also interfaces to the same LAPACK libraries used by their NumPy equivalents, but offer many additional parameters to tune how the calculation is done, and what is returned.\n",
    "\n",
    "- [scipy.linalg.eigvals](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigvals.html) calculates eigenvalues, and [scipy.linalg.eig](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig.html) calculates eigenvalues and eigenvectors.\n",
    "- [scipy.linalg.eigvalsh](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigvalsh.html) and [scipy.linalg.eigh](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html) do the same thing for real symmetric or Hermitian matrices.\n",
    "- In addition recent SciPy versions (you may need to update via pip: `sudo pip3 install scipy --upgrade`) have dedicated functions for:\n",
    "    - Real symmetric or Hermitian banded matrices  [scipy.linalg.eigvals_banded](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigvals_banded.html) and [scipy.linalg.eig.banded](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eig_banded.html)\n",
    "    - Real symmetric tridiagonal matrices [scipy.linalg.eigh_tridiagonal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh_tridiagonal.html) and [scipy.linalg.eigh_tridiagonal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh_tridiagonal.html) \n",
    "- There is also QR decomposition with [scipy.linalg.qr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.qr.html) and many related functions that offer more advanced options relative to the NumPy version, such as the ability to also generate a pivot matrix which orders the diagonal elements of R.\n",
    "\n",
    "While neither NumPy or SciPy have a function specifically for a leading eigenvalue/vector as obtained from power iteration, the SciPy eigvalsh function can take an `eigvals` argument to restrict the calculation to only calculate certain eigenvalues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.98856236e+01+0.j        ,  2.45762799e+00+1.66031308j,\n",
       "        2.45762799e+00-1.66031308j,  2.83612563e+00+0.j        ,\n",
       "       -3.03799994e+00+0.28214241j, -3.03799994e+00-0.28214241j,\n",
       "        1.17344414e+00+2.59688187j,  1.17344414e+00-2.59688187j,\n",
       "       -1.74336835e+00+2.35370075j, -1.74336835e+00-2.35370075j,\n",
       "        1.76212341e+00+2.05604446j,  1.76212341e+00-2.05604446j,\n",
       "        2.60576559e+00+0.70224999j,  2.60576559e+00-0.70224999j,\n",
       "       -2.43334105e+00+1.04862719j, -2.43334105e+00-1.04862719j,\n",
       "       -2.19626369e+00+1.36286264j, -2.19626369e+00-1.36286264j,\n",
       "       -2.21363250e-01+2.56135291j, -2.21363250e-01-2.56135291j,\n",
       "       -1.24938543e-02+2.54720771j, -1.24938543e-02-2.54720771j,\n",
       "       -2.01551856e+00+1.55741701j, -2.01551856e+00-1.55741701j,\n",
       "        2.58609103e+00+0.j        , -8.82181423e-01+2.34225884j,\n",
       "       -8.82181423e-01-2.34225884j, -2.47630910e+00+0.j        ,\n",
       "        2.00749457e+00+1.37904585j,  2.00749457e+00-1.37904585j,\n",
       "        5.19946758e-01+2.28416324j,  5.19946758e-01-2.28416324j,\n",
       "        6.26694480e-01+2.22195302j,  6.26694480e-01-2.22195302j,\n",
       "       -2.26649660e+00+0.34290507j, -2.26649660e+00-0.34290507j,\n",
       "       -1.03756983e+00+2.04511289j, -1.03756983e+00-2.04511289j,\n",
       "        1.03949380e+00+1.91690762j,  1.03949380e+00-1.91690762j,\n",
       "        2.06134403e+00+0.j        ,  2.08380768e+00+0.61563186j,\n",
       "        2.08380768e+00-0.61563186j,  1.85392811e+00+1.09134089j,\n",
       "        1.85392811e+00-1.09134089j, -1.29031389e+00+1.66388215j,\n",
       "       -1.29031389e+00-1.66388215j, -1.01050150e+00+1.80168412j,\n",
       "       -1.01050150e+00-1.80168412j, -1.75215721e+00+1.01349215j,\n",
       "       -1.75215721e+00-1.01349215j,  1.10643777e+00+1.58470209j,\n",
       "        1.10643777e+00-1.58470209j,  3.53204416e-01+1.86507279j,\n",
       "        3.53204416e-01-1.86507279j, -1.75151205e+00+0.66387935j,\n",
       "       -1.75151205e+00-0.66387935j, -6.72429021e-01+1.75375908j,\n",
       "       -6.72429021e-01-1.75375908j, -1.92848314e+00+0.j        ,\n",
       "       -1.12896520e+00+1.36613308j, -1.12896520e+00-1.36613308j,\n",
       "       -2.20311766e-01+1.73344866j, -2.20311766e-01-1.73344866j,\n",
       "        1.58749927e+00+0.91386755j,  1.58749927e+00-0.91386755j,\n",
       "       -2.91907025e-02+1.72136279j, -2.91907025e-02-1.72136279j,\n",
       "        1.75876779e+00+0.j        ,  7.68334218e-01+1.3532655j ,\n",
       "        7.68334218e-01-1.3532655j , -1.39334449e+00+0.56174188j,\n",
       "       -1.39334449e+00-0.56174188j, -1.48699138e+00+0.j        ,\n",
       "       -1.09457494e+00+0.80890264j, -1.09457494e+00-0.80890264j,\n",
       "        1.49999276e+00+0.j        ,  1.27296218e+00+0.63997876j,\n",
       "        1.27296218e+00-0.63997876j, -9.25802890e-02+1.19061397j,\n",
       "       -9.25802890e-02-1.19061397j,  1.32744287e+00+0.21899257j,\n",
       "        1.32744287e+00-0.21899257j,  5.30570068e-01+0.j        ,\n",
       "        9.98499846e-01+0.68018872j,  9.98499846e-01-0.68018872j,\n",
       "       -1.01501513e+00+0.30760976j, -1.01501513e+00-0.30760976j,\n",
       "       -4.49581528e-01+0.94900743j, -4.49581528e-01-0.94900743j,\n",
       "        2.95854084e-01+0.81281958j,  2.95854084e-01-0.81281958j,\n",
       "        6.40471951e-01+0.84329627j,  6.40471951e-01-0.84329627j,\n",
       "       -3.82647864e-01+0.40674355j, -3.82647864e-01-0.40674355j,\n",
       "        3.72827032e-01+0.j        ,  2.51786244e-01+0.47823989j,\n",
       "        2.51786244e-01-0.47823989j, -2.43621487e-01+0.j        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvals(rand_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.98856236e+01+0.j        ,  2.45762799e+00+1.66031308j,\n",
       "        2.45762799e+00-1.66031308j,  2.83612563e+00+0.j        ,\n",
       "       -3.03799994e+00+0.28214241j, -3.03799994e+00-0.28214241j,\n",
       "        1.17344414e+00+2.59688187j,  1.17344414e+00-2.59688187j,\n",
       "       -1.74336835e+00+2.35370075j, -1.74336835e+00-2.35370075j,\n",
       "        1.76212341e+00+2.05604446j,  1.76212341e+00-2.05604446j,\n",
       "        2.60576559e+00+0.70224999j,  2.60576559e+00-0.70224999j,\n",
       "       -2.43334105e+00+1.04862719j, -2.43334105e+00-1.04862719j,\n",
       "       -2.19626369e+00+1.36286264j, -2.19626369e+00-1.36286264j,\n",
       "       -2.21363250e-01+2.56135291j, -2.21363250e-01-2.56135291j,\n",
       "       -1.24938543e-02+2.54720771j, -1.24938543e-02-2.54720771j,\n",
       "       -2.01551856e+00+1.55741701j, -2.01551856e+00-1.55741701j,\n",
       "        2.58609103e+00+0.j        , -8.82181423e-01+2.34225884j,\n",
       "       -8.82181423e-01-2.34225884j, -2.47630910e+00+0.j        ,\n",
       "        2.00749457e+00+1.37904585j,  2.00749457e+00-1.37904585j,\n",
       "        5.19946758e-01+2.28416324j,  5.19946758e-01-2.28416324j,\n",
       "        6.26694480e-01+2.22195302j,  6.26694480e-01-2.22195302j,\n",
       "       -2.26649660e+00+0.34290507j, -2.26649660e+00-0.34290507j,\n",
       "       -1.03756983e+00+2.04511289j, -1.03756983e+00-2.04511289j,\n",
       "        1.03949380e+00+1.91690762j,  1.03949380e+00-1.91690762j,\n",
       "        2.06134403e+00+0.j        ,  2.08380768e+00+0.61563186j,\n",
       "        2.08380768e+00-0.61563186j,  1.85392811e+00+1.09134089j,\n",
       "        1.85392811e+00-1.09134089j, -1.29031389e+00+1.66388215j,\n",
       "       -1.29031389e+00-1.66388215j, -1.01050150e+00+1.80168412j,\n",
       "       -1.01050150e+00-1.80168412j, -1.75215721e+00+1.01349215j,\n",
       "       -1.75215721e+00-1.01349215j,  1.10643777e+00+1.58470209j,\n",
       "        1.10643777e+00-1.58470209j,  3.53204416e-01+1.86507279j,\n",
       "        3.53204416e-01-1.86507279j, -1.75151205e+00+0.66387935j,\n",
       "       -1.75151205e+00-0.66387935j, -6.72429021e-01+1.75375908j,\n",
       "       -6.72429021e-01-1.75375908j, -1.92848314e+00+0.j        ,\n",
       "       -1.12896520e+00+1.36613308j, -1.12896520e+00-1.36613308j,\n",
       "       -2.20311766e-01+1.73344866j, -2.20311766e-01-1.73344866j,\n",
       "        1.58749927e+00+0.91386755j,  1.58749927e+00-0.91386755j,\n",
       "       -2.91907025e-02+1.72136279j, -2.91907025e-02-1.72136279j,\n",
       "        1.75876779e+00+0.j        ,  7.68334218e-01+1.3532655j ,\n",
       "        7.68334218e-01-1.3532655j , -1.39334449e+00+0.56174188j,\n",
       "       -1.39334449e+00-0.56174188j, -1.48699138e+00+0.j        ,\n",
       "       -1.09457494e+00+0.80890264j, -1.09457494e+00-0.80890264j,\n",
       "        1.49999276e+00+0.j        ,  1.27296218e+00+0.63997876j,\n",
       "        1.27296218e+00-0.63997876j, -9.25802890e-02+1.19061397j,\n",
       "       -9.25802890e-02-1.19061397j,  1.32744287e+00+0.21899257j,\n",
       "        1.32744287e+00-0.21899257j,  5.30570068e-01+0.j        ,\n",
       "        9.98499846e-01+0.68018872j,  9.98499846e-01-0.68018872j,\n",
       "       -1.01501513e+00+0.30760976j, -1.01501513e+00-0.30760976j,\n",
       "       -4.49581528e-01+0.94900743j, -4.49581528e-01-0.94900743j,\n",
       "        2.95854084e-01+0.81281958j,  2.95854084e-01-0.81281958j,\n",
       "        6.40471951e-01+0.84329627j,  6.40471951e-01-0.84329627j,\n",
       "       -3.82647864e-01+0.40674355j, -3.82647864e-01-0.40674355j,\n",
       "        3.72827032e-01+0.j        ,  2.51786244e-01+0.47823989j,\n",
       "        2.51786244e-01-0.47823989j, -2.43621487e-01+0.j        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.eigvals(rand_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Linear Algebra\n",
    "\n",
    "As you might imagine, certain algorithms are more optimal for certain types of system. Sparse matrices (where most of the elements are zero) are special enough, and occur commonly enough, that there is an entire SciPy module for space linear algebra: [scipy.linalg.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html) separate from the main linear algebra module.\n",
    "\n",
    "Many models can result in sparse matrices: for example finite element method solutions of partial differential equations.\n",
    "\n",
    "The module has many linear system solvers for problems such as we saw previously, and functions to find a particular number of eigenvalues of the matrix: [scipy.sparse.linalg.eigs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html), or all eigenvalues/vectors: [scipy.sparse.linalg.lobpcg](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lobpcg.html).\n",
    "\n",
    "If you have represented your problem with a sparse matrix, these functions will be significantly faster. Also, while you can pass arrays in the usual array format, this can be wasteful as most elements are zero. For this reason, there are different ways to represent your matrix that are much more compact. These are given as classes in [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
