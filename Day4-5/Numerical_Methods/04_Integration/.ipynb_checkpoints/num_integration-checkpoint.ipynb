{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This time we'll be using\n",
    "import scipy.integrate\n",
    "import scipy.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Integration\n",
    "\n",
    "It's quite common to have a some set of data from a calculation that doesn't follow any analytic behaviour but you need to find the area or volume under it, or we may have some expression where the integral is difficult to calculate analytically. It's useful to be able to calculate integrals numerically for this reason.\n",
    "\n",
    "There are are advanced methods available if we know the function we are integrating, than those we can use when  we have a fixed set of samples. All methods for the latter case can be used for the former case however so we'll look at those first.\n",
    "\n",
    "For example, we have a file called \"si_dos.dat\" which is a calculation of the electronic density of states for silicon. As a check we want to make sure it integrates to the correct number. Let's look at our data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si_dos = np.loadtxt(\"si_dos_tet.dat\")\n",
    "plt.plot(si_dos[:,0], si_dos[:,1])\n",
    "plt.xlabel('Energy (eV)')\n",
    "plt.ylabel('Electronic Density of States')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemann sum\n",
    "\n",
    "The simplest thing to do is calculate a left or right Riemann sum, where we simply take a narrow rectangle assigning a y-value to each x interval and add all their areas. If we have obtained $f(x_i)$ for a set of regularly spaced $x_i$ values ($x_{i+1} = x_i + \\Delta x$) with $i$ from $0$ to $n$, then this is just a matter of calculating\n",
    "$$ \\Sigma_{i=0}^{n-1} f(x_i) \\Delta x $$ for the left Riemann sum, or\n",
    "$$ \\Sigma_{i=1}^n f(x_i) \\Delta x $$ for the right Riemann sum.\n",
    "\n",
    "As we can see these are identical except for the starting and ending points. We can construct functions to calculate these.\n",
    "\n",
    "Note - this is equivalent to taking using a constant interpolating function as we saw previously and integrating under this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def left_riemann(dx, yvals):\n",
    "    '''Calculate the area under y(x) as a left Riemann sum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dx - the spacing between x-values\n",
    "    yvals - the value of y(x) at each x\n",
    "    '''\n",
    "    area = 0.0\n",
    "    for i in range(len(yvals)-1):\n",
    "        area += dx * yvals[i]\n",
    "    return area\n",
    "\n",
    "# Lets some simple optimizations here - since dx is constant in the sum we can\n",
    "# add up the appropriate y-values and multiply by dx at the end.\n",
    "# Then we can also use a numpy vector operation.\n",
    "def right_riemann(dx, yvals):\n",
    "    '''Calculate the area under y(x) as a right Riemann sum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dx - the spacing between x-values\n",
    "    yvals - the value of y(x) at each x\n",
    "    '''\n",
    "    area = np.sum(yvals[1:]) * dx\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si_dos_dx = si_dos[1,0]-si_dos[0,0]\n",
    "print(left_riemann(si_dos_dx, si_dos[:,1]))\n",
    "print(right_riemann(si_dos_dx, si_dos[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit left_riemann(si_dos_dx, si_dos[:,1])\n",
    "%timeit right_riemann(si_dos_dx, si_dos[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The difference between the last and first rectangle that can be constructed\n",
    "print(si_dos_dx * si_dos[-1,1] - si_dos_dx * si_dos[0,1])\n",
    "\n",
    "# The difference between the right and left riemann sum\n",
    "print(right_riemann(si_dos_dx, si_dos[:,1]) - left_riemann(si_dos_dx, si_dos[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trapezoidal Integration\n",
    "\n",
    "Rather than taking a rectangle at each interval, we could take a trapezoid, which is equivalent to linearly interpolating our data before integrating. To calculate the area of this, we could take the area of say the left Riemann rectangle and add or subtract the area of triangle connecting it the next y-value. However, as we can see in the image below, the area is equivalent to a rectangle with it's height given by the average of the neighboring y-values. \n",
    "\n",
    "![Trapezoidal integration](images/trapezoidal.png)\n",
    "\n",
    "This means we can express the area as:\n",
    "$$ \\Sigma_{i=0}^{n-1} \\frac{f(x_i) + f(x_{i+1})}{2} \\Delta x. $$\n",
    "\n",
    "This can then be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trapezoidal(dx, yvals):\n",
    "    '''Calculate the area under y(x) using trapezoidal integration\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dx - the spacing between x-values\n",
    "    yvals - the value of y(x) at each x\n",
    "    '''\n",
    "    y_sum = 0.0\n",
    "    for i in range(len(yvals)-1):\n",
    "        y_sum += yvals[i] + yvals[i+1]\n",
    "    return y_sum * 0.5 * dx # We can take the 1/2 outside the sum as with the dx.\n",
    "\n",
    "# Again let's try to vectorize this\n",
    "def trapezoidal_np(dx, yvals):\n",
    "    '''Calculate the area under y(x) using trapezoidal integration\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dx - the spacing between x-values\n",
    "    yvals - the value of y(x) at each x\n",
    "    '''\n",
    "    area = np.sum(yvals[:-1]+yvals[1:]) * 0.5 * dx\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trapezoidal(si_dos_dx, si_dos[:,1]))\n",
    "print(trapezoidal_np(si_dos_dx, si_dos[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit trapezoidal(si_dos_dx, si_dos[:,1])\n",
    "%timeit trapezoidal_np(si_dos_dx, si_dos[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The trapezoidal rule corresponds to the average of the left and right Riemann sum\n",
    "0.5*(left_riemann(si_dos_dx, si_dos[:,1]) + right_riemann(si_dos_dx, si_dos[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our implementation of the trapezoidal method again. We already made one small optimization by not dividing by two for every single step and recognizing this factor could be taken outside the sum (and as multiplying is faster than dividing, it's usually good to get in the habit of casting any constant factors as multiplying factors), and we also vectorized it using numpy arrays, which made a huge difference.\n",
    "\n",
    "As we loop over the intervals you may notice we are counting all values twice except at the start and end points and then dividing by two at the end. So you may have the feeling we are doing more than we need to here. We can see this also if we think of the trapezoidal method as the average of the left and right Riemann sums. These methods are identical execpt the left includes the rectangular area $\\Delta x * f(x_0)$ while the right includes instead the rectangular area $\\Delta x * f(x_n)$.\n",
    "\n",
    "Let's try another implementation that takes this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trapezoidal_np2(dx, yvals):\n",
    "    '''Calculate the area under y(x) using trapezoidal integration\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dx - the spacing between x-values\n",
    "    yvals - the value of y(x) at each x\n",
    "    '''\n",
    "    \n",
    "    ysum = 0.5 * (yvals[0]+yvals[-1])\n",
    "    ysum += np.sum(yvals[1:-1])\n",
    "    area = ysum * dx\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(trapezoidal_np2(si_dos_dx, si_dos[:,1]))\n",
    "%timeit trapezoidal_np2(si_dos_dx, si_dos[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing this with NumPy and SciPy\n",
    "\n",
    "Numpy has an implementation of trapezoidal integration that can be used as [numpy.trapz](https://docs.scipy.org/doc/numpy/reference/generated/numpy.trapz.html), and there is also a scipy version in [scipy.integrate](https://docs.scipy.org/doc/scipy/reference/integrate.html) as [scipy.integrate.trapz](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapz.html). These routines are a little more advanced than what we've writte so far, as they can also be used with an explicit set of x-values.\n",
    "\n",
    "They could be used with our example data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.trapz(si_dos[:, 1], dx=si_dos_dx))\n",
    "print(scipy.integrate.trapz(si_dos[:, 1], dx=si_dos_dx))\n",
    "\n",
    "%timeit np.trapz(si_dos[:, 1], dx=si_dos_dx)\n",
    "%timeit scipy.integrate.trapz(si_dos[:, 1], dx=si_dos_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already saw the left and right Riemann sums could be calculated as sums over numpy array expressions as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lRsum = si_dos[:-1, 1].sum() * si_dos_dx\n",
    "rRsum = si_dos[1:, 1].sum() * si_dos_dx\n",
    "print(\"lR sum\", lRsum, \" rRsum\", rRsum, \" Average\", 0.5*(lRsum+rRsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpson's Rule\n",
    "\n",
    "Consider what we've done in terms of integrating under some interpolation of the function we have sampled. We could think of the left and right Riemann sums as approximating the function as constant valued in each segment. Then the trapezoidal integration, is a linear interpolation between the the function evaluated on the border of each segment. The natural next step could then be to use a higher order interpolation.\n",
    "\n",
    "If we have sampled our function to be integrated such that we have an even number of intervals that are equally spaced (i.e odd number of sampled points), then we can take our points in groups of three, find the quadratic that passes through those three points and use the area under this curve as the area for this (double-interval) section.\n",
    "\n",
    "This procedure is known as Simpon's rule. Say we have 3 evenly spaced points: $(-h, y_{-h})$, $(0, y_0)$ and $(h, y_h)$ and we want to find the area under the quadratic that passes through them. First let's examine the area of a general quadratic function $f(x) = a+bx+cx^2$ between $-h$ and $h$:\n",
    "\n",
    "$$\n",
    "\\int_{-h}^{h} a+bx+cx^2 = [ax + bx^2/2 + cx^3/3]_{-h}^{h} \\\\\n",
    "= ah+bh^2/2+ch^3/3\\quad+ah-bh^2/2+ch^3/3 = 2ah+2ch^3/3\\\\\n",
    "= \\frac{h}{3}(6a+2ch^2)\n",
    "$$\n",
    "\n",
    "And we can get $y_{-h} = a-bh+ch^2$, $y_0=f(0)=a$, and $y_h=a+bh+ch^2$, from which we can find that $6a+2ch^2=y_{-h}+4y_0+y_h$. This means that our integral is now given by\n",
    "\n",
    "$$\n",
    "\\frac{h}{3}(y_{-h}+4y_0+y_h).\n",
    "$$\n",
    "\n",
    "So if we have have an even number of sampled intervals (odd number of points), we can pair up the intervals and quickly find the integral of the quadratic that passes through the sampled points forming that pair of intervals.\n",
    "\n",
    "We can write this as a set of weightings applied to each sampled y-value, and then add them all up. The end points have a weighting of 1, even points a weighting of 4 (these are in the centre of our intervals), and the other odd points a weighting of 2 (these get one from each interval they are part of). In Python however, indexing starts from 0 rather than 1 so be careful setting this up.\n",
    "\n",
    "Note - you may also see Simpson's rule given with a factor of 6. This is usually the case in the context of integrating a sampled function, where the width of the full interval spanned is used: $2h$ instead of the interval between adjacent sampled points $h$.\n",
    "\n",
    "Let's write a Python function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpsons(dx, yvals):\n",
    "    '''Simpsons Rule'''\n",
    "    assert len(yvals) % 2 != 0, \"Odd number of sampled points required\"\n",
    "    \n",
    "    ysum = yvals[0] + yvals[-1] # End points\n",
    "    for i in range(1, len(yvals)-1):\n",
    "        if i%2 == 0:\n",
    "            ysum += 2*yvals[i]\n",
    "        else:\n",
    "            ysum += 4*yvals[i]\n",
    "    area = ysum * dx / 3.0\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(simpsons(si_dos_dx, si_dos[:,1]))\n",
    "%timeit simpsons(si_dos_dx, si_dos[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an explicit for loop there, so let's try to vectorize this. A useful feature of NumPy array indexing is that it allows us to set the stride. We have used this previously to reverse the order of an array. We can use it here to quickly pick out all the odd or even elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "print(a[1:-1:2])\n",
    "print(a[2:-1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpsons_np(dx, yvals):\n",
    "    '''Simpsons Rule'''\n",
    "    assert len(yvals)%2 != 0, \"Odd number of sampled points required\"\n",
    "    \n",
    "    ysum = yvals[0] + yvals[-1] # End points\n",
    "    ysum += 4*np.sum(yvals[1:-1:2]) # Even points\n",
    "    ysum += 2*np.sum(yvals[2:-1:2]) # Odd points\n",
    "    area = ysum * dx / 3.0\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(simpsons_np(si_dos_dx, si_dos[:,1]))\n",
    "%timeit simpsons_np(si_dos_dx, si_dos[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing this with SciPy\n",
    "\n",
    "SciPy has the [scipy.integrate.simps](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simps.html) function that can be used to integrate with Simpson's rule. You can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(scipy.integrate.simps(si_dos[:,1], dx=si_dos_dx))\n",
    "%timeit scipy.integrate.simps(si_dos[:,1], dx=si_dos_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating under a spline\n",
    "\n",
    "As we saw previously, a cubic spline can be used to give a nice smooth interpolation of some sampled data. As you might imagine, this can be used to estimate the integral also. Since the spline generates a set of coefficients of a cubic polynomial for each interval, the exact integral _of the spline_ can be found. For a very jagged set of sampled data, such as we have for a density of states obtained with the tetrahedron method, this is not likely to actually be a better representation of the actual integral than a simpler approach such as the trapezoidal method.\n",
    "\n",
    "If you have generated a spline with the [scipy.interpolate.InterpolatedUnivariateSpline](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.InterpolatedUnivariateSpline.html) class, you can use its [integral](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.InterpolatedUnivariateSpline.integral.html) method to find the integral under it.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_interp =  scipy.interpolate.InterpolatedUnivariateSpline(si_dos[:,0], si_dos[:,1])\n",
    "print(us_interp.integral(si_dos[0, 0], si_dos[-1, 0]))\n",
    "\n",
    "# The time consuming step would actually be generating the spline, as this involves solving a linear system\n",
    "# so once the interpolation is found, calculating the integral is quite fast.\n",
    "%timeit us_interp.integral(si_dos[0, 0], si_dos[-1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-Cotes Quadrature\n",
    "\n",
    "The Newton-Cotes quadrature rules are a group of approaches to calculate integrals numerically (quadrature) that are based on evaluating the integral at a set of evenly spaced points. They involve expressing the integral as a sum over the the evaluations of the function at each point times a weight. The weights are be found using different polynomial interpolations, but there is a risk of Runge's phenomenon for high order polynomial as discussed in earlier lessons.\n",
    "\n",
    "If we're trying to integrate a function $f(x)$ from $x=a$ to $x=b$, and we sample $f(x)$ to $n+1$ times (inclusive, in what's known as the closed form) in the interval, we can express the integral as\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)dx = \\Sigma_{i=0}^n w_i f_i\n",
    "$$\n",
    "\n",
    "with $f_i$ indicating $f(x_i)$. Several of the approaches we used earlier involve applying this to the multiple intervals found in our sampled data. Here are some formulas for a the contribution from a particular interval or set of intervals. If implementing them, as you break your sample up into the appropriate groups of intervals, you need to be sure add the weights correctly for points that are on the right of one region and the left of another. For example, when we add all our segments together only the boundary points are weighted by 0.5 in the trapezoidal method, and odd internal points have double the weighting in Simpson's method.\n",
    "\n",
    "| Degree |   Method      | Formula                           |\n",
    "|--------|---------------|-----------------------------------|\n",
    "|    1   | Trapezoidal   | $\\frac{b-a}{2}(f_0+f_1)$          |\n",
    "|    2   | Simpson's     | $\\frac{b-a}{6}(f_0+4f_1+f_2)$     |\n",
    "|    3   | Simpson's 3/8 | $\\frac{b-a}{8}(f_0+3f_1+3f_2+f_3)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for integrating a known function\n",
    "\n",
    "In all the methods we've discussed so far we considered the case of integrating a set of sampled data. All these approaches can also be used for integrating a known function. However there are other methods available in this case that may be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Errors\n",
    "\n",
    "Generally speaking, as you might imagine, the best way to reduce the error of your interpolation is add more sampled points. Different methods will however differ significantly in how quickly the error reduces with the addition of more sampled points, in quite a well defined way (provided you are integrating a smooth function). While we won't go into the details of how these are calculated, there is an overview [here](https://en.wikipedia.org/wiki/Numerical_integration#Conservative_.28a_priori.29_error_estimation), and in the calculation of simpler approximations [here](https://en.wikipedia.org/wiki/Riemann_sum).\n",
    "\n",
    "Here are some of the error terms for the Newton-Cotes methods given above where $f^{(2)}(\\xi)$ indicates the second derivative of $f(x)$ for some number between the limits of the interval $a$ and $b$.\n",
    "\n",
    "- Trapezoidal:   $-\\frac{(b-a)^3}{12}f^{(2)}(\\xi)$ \n",
    "- Simpson's:    $-\\frac{(b-a)^5}{2880}f^{(4)}(\\xi)$\n",
    "- Simpson's 3/8: $-\\frac{(b-a)^5}{6480}f^{(4)}(\\xi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Midpoint Rule\n",
    "\n",
    "This is quite similar to the trapezoidal rule. In the trapezoidal rule, you integrate under a straight line connecting the y-values at the boundaries of the interval, which as we saw corresponds to taking the area as a rectangle with a height half way between the two y-values. In the midpoint rule, you instead take the area as a rectangle with height given by the y-value found by evaluating the function at the value of x half way between the two boundary values. An interesting, and perhaps unexpected, property is that for a known function, this tends to have a lower error than the trapezoidal rule. There is a nice discussion of this on [stack exchange](https://math.stackexchange.com/questions/603830/why-does-trapezoidal-rule-have-potential-error-greater-than-midpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Quadrature\n",
    "\n",
    "Say we have a case where we're trying to integrate some function that is quite expensive to calculate, so ideally we'd like to minimize the number of samples we need. In contrast to the Newton-Cotes methods mentioned earlier, in which a fixed size interval is used, in Gaussian quadrature both the sampling points and their weights are chosen. In fact an $n$-point Gaussian quadrature, is constructed so as to give an exact result for polynomials of degree $2n-1$ or less.\n",
    "\n",
    "Let's start by supposing we're trying to evaluate some integral between 0 and 1, and we want to approximate this as a sum over the function evaluated at some $n$ sampled points multiplied by some weighting:\n",
    "\n",
    "$$\n",
    "I = \\int_{-1}^{1} f(x) dx \\approx \\Sigma_{i=1}^n w_i f(x_i).\n",
    "$$\n",
    "\n",
    "So we want to find the distribution of samples and weights that will make this approximation as accurate as possible. Let's suppose we can approximate our function in this interval by an $m^{th}$ order polynomial $P_m(x)$, then we can write\n",
    "\n",
    "$$\n",
    "I \\approx \\int_{-1}^1 P_m(x)dx=\\int_{-1}^1\\left(a_0+a_1x+a_2x^2+\\cdots+a_mx^m\\right)dx\\\\\n",
    "=2a_0+\\frac{2}{3}a_2+\\frac{2}{5}a_4+\\cdots+\\frac{2}{m+1}a_m\n",
    "$$\n",
    "\n",
    "As we said, we wish to express this as the sum given above, so\n",
    "\n",
    "$$\n",
    "I \\approx \\Sigma_{i=1}^n w_iP_m(x_i) = \\Sigma_{i=1}^n w_i\\left(a_0+a_1x+a_2x^2+\\cdots+a_mx^m\\right)\\\\\n",
    "=a_0\\Sigma_{i=1}^nw_i +a_1\\Sigma_{i=1}^nw_ix_i+a_2\\Sigma_{i=1}^nw_ix_i^2+\\cdots+a_m\\Sigma_{i=1}^nw_ix_i^m.\n",
    "$$\n",
    "\n",
    "Equating these expressions term by term, we obtain the following set of equations:\n",
    "\n",
    "$$\n",
    "\\Sigma w_i=2\\\\\n",
    "\\Sigma w_ix_i=0\\\\\n",
    "\\Sigma w_ix_i^2=\\frac{2}{3}\\\\\n",
    "\\Sigma w_ix_i^3=0\\\\\n",
    "\\vdots\n",
    "$$\n",
    "\n",
    "giving us $m+1$ equations in $2n$ unknowns. We can chose both $n$ and $m$, so we can set $m+1=2n$ and find the optimal set of weights and samples.\n",
    "\n",
    "Say we choose a single sample ($n=1$), this means $m=1$, and we must solve the first two equations above. The first tells us $w_1=2$, and the second that $x_1=0$. So this means we approximate the integral using a single sample at the midpoint, and is equivalent to the midpoint rule.\n",
    "\n",
    "Say we choose two samples ($n=2$ and $m=3$); we must now solve the first four equations above. Written out explicitly, these will be:\n",
    "\n",
    "$$\n",
    "w_1+w_2 = 2\\\\\n",
    "w_1x_1 + w_2x_2 = 0\\\\\n",
    "w_1x_1^2 + w_2x_2^2 = 2/3\\\\\n",
    "w_1x_1^3 + w_2x_2^3 = 0\n",
    "$$\n",
    "\n",
    "Solving these, we'll find that $x_1 = -\\frac{1}{\\sqrt{3}}$, $x_2=\\frac{1}{\\sqrt{3}}$, $w_1=1$, and $w_2=2$. So with just two samples, found and weighted appropriately, our integration in the interval will be exact if the function we're trying to integrate is a cubic or lower order polynomial!\n",
    "\n",
    "We can keep going in this manner and find parameters for higher order interpolating polynomials. For example, for $n=3$ you obtain $x_i = \\left\\{-\\sqrt{\\frac{3}{5}}, 0, \\sqrt{\\frac{3}{5}}\\right\\}$ and $w_i=\\left\\{\\frac{5}{9}, \\frac{8}{9}, \\frac{5}{9}\\right\\}$.\n",
    "\n",
    "The error of a Gaussian quadrature rule is proportional to $\\frac{f^{(2n)}(\\xi)}{(2n)!)}$.\n",
    "\n",
    "The roots and weights can in-fact be obtained from the [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials), so the method above with $w_i=1$ is often known as Gauss-Legendre quadrature.\n",
    "\n",
    "So far, we've restricted ourselves to the interval $[-1, 1]$, however any definite integral can be transformed to one on this range as follows:\n",
    "\n",
    "$$\n",
    "\\int_a^b f(x)dx = \\frac{b-a}{2}\\int_{-1}^1 f\\left(\\frac{b-a}{2}x + \\frac{a+b}{2}\\right)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In SciPy\n",
    "\n",
    "\n",
    "The [scipy.integrate.fixed_quad](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.fixed_quad.html) function can be used to calculate a fixed-order Gaussian quadrature, defaulting to 5th order. This is a Python implementation. Try taking a look at the source, linked from the documentation page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's just integrate a Sin^2 function between 0 and 1\n",
    "def sin2(x):\n",
    "    return np.sin(x)**2\n",
    "\n",
    "# This returns two values, the integral and None.\n",
    "print(scipy.integrate.fixed_quad(sin2, 0, 2*np.pi, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how the value converges to the expected value with increasing order\n",
    "\n",
    "orders = np.arange(5, 11) # Go from 5 to 10\n",
    "errors = [np.abs(np.pi - scipy.integrate.fixed_quad(sin2, 0, 2*np.pi, n=order)[0]) for order in orders]\n",
    "plt.semilogy(orders, errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to know an integral to a particular accuracy, the Gaussian quadrature method gives us a prescription for how to increase our sampling in such a way as to maximize the accuracy. We can keep increasing the order of the interpolation until the value doesn't change within some tolerance, or the relative change between subsequent values is less than some tolerance (in case the integral has a small absolute value). We saw in the last Python example that this could be done with a loop over calls to the scipy `fixed_quad` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [scipy.integrate.quadrature](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quadrature.html) function can be used to perform adaptive Gaussian quadrature to evaluate the integral of a function, exactly as outlined above. By default it will keep increasing the order of the quadrature until the integral is converged to within some tolerance (either absolute _or_ relative), within optionally specified limits of quadrature order.\n",
    "\n",
    "Here's a simple Python example of their use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will return two values: the integral, and the difference between the last two estimates\n",
    "result, error = scipy.integrate.quadrature(sin2, 0, 2*np.pi)\n",
    "\n",
    "print(\"Integral=\", result, \", Error=\", error, \", Difference to known result=\", np.pi-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Quadrature\n",
    "\n",
    "In the previously discussed approaches, the choices of interval or sampled points, were either fixed or set by some general rules. However, neither of these approaches take account of how the function is actually behaving. If the function is very smooth in a subinterval, we don't need many samples there to have a good representation of the integral. While if the function varies very rapidly in a subinterval, we should sample this more densely to accurately find the integral of this region.\n",
    "\n",
    "This is the idea behind approaches that are generally known as adaptive quadrature. While there are many variants, the general procedure for finding the integral of a function $f(x)$ over a region $[a, b]$ to within a tolerance $t$ is as follows:\n",
    "- Evaluate the approximate integral over $[a, b]$ with some fixed quadrature rule, and estimate the error.\n",
    "- If the error is bigger than $t$, divide the interval in two parts $[a, m]$ and $[m, b]$\n",
    "- Now approximate the integral as the integral over $[a, m]$ to within tolerance $t/2$ plus the integral over $[m, b]$ to within tolerance $t/2$, with both of these integrals calculated using this same procedure.\n",
    "- This can be implemented recursively (when a function calls itself), and will lead to parts of the function which vary most rapidly being sampled more densely than those which vary smoothly.\n",
    "- How the error is estimated, and the method used to approximate the integral on an interval will vary between implementations.\n",
    "    - Often it's easiest to estimate the error associated with an interval as the difference between the integral estimate for the full interval and the integral estimate obtained from the sum of the estimates after dividing it in two.\n",
    "    \n",
    "Let's write a Python function that will do this with the midpoint method, using the error estimate described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def midpoint_rule(func, a, b):\n",
    "    return func(0.5*(a+b)) * np.abs(b-a)\n",
    "\n",
    "def adaptive_mid(func, a, b, tol=1e-6):\n",
    "    '''Adaptive midpoint integration.'''\n",
    "\n",
    "    m = 0.5*(a+b)\n",
    "    approx1 = midpoint_rule(func, a, b)\n",
    "    approx2 = midpoint_rule(func, a, m) + midpoint_rule(func, m, b)\n",
    "\n",
    "    if np.abs(approx2 - approx1) > tol:\n",
    "        left = adaptive_mid(func, a, m, 0.5*tol)\n",
    "        right = adaptive_mid(func, m, b, 0.5*tol)\n",
    "        approx2 = left + right\n",
    "\n",
    "    # approx2 should be a better estimate so return that.\n",
    "    return approx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaptive_mid(sin2, 0, 2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit adaptive_mid(sin2, 0, np.pi*0.2)\n",
    "%timeit scipy.integrate.quadrature(sin2, 0, np.pi*0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice, our implementation above evaluates $f(x)$ more times than is strictly necessary.\n",
    "- Can you see why?\n",
    "- How might you improve this?\n",
    "\n",
    "This recursive approach is not so amenable to the other methods we discussed to vectorization, which means it's hard to take advantage of NumPy to improve the speed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In SciPy\n",
    "\n",
    "There is the [scipy.integrate.quad](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html#scipy.integrate.quad) function which calls the QUADPACK Fortran library to perform adaptive quadrature. It has many options available, and uses [Clenshaw-Curtis quadrature](https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature) in each sub-interval. This offers a similar level of accuracy to Gaussian quadrature but is more suited for a recursive subdivision of intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By default this also outputs the result and the error estimate.\n",
    "# Note the error estimate is calculated differently than the quadrature function.\n",
    "scipy.integrate.quad(sin2, 0, 2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit scipy.integrate.quadrature(sin2, 0, 2*np.pi)\n",
    "%timeit scipy.integrate.quad(sin2, 0, 2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enabling \"full_output\" will give many additional details as\n",
    "# a third output of the function call.\n",
    "result = scipy.integrate.quad(sin2, 0, 2*np.pi, full_output=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An explaination of what the values in the output dictionary\n",
    "# mean can be generated with the following function.\n",
    "scipy.integrate.quad_explain()\n",
    "\n",
    "# So we can see that the value of \"last=1\" means it didn't need\n",
    "# to subdivide the interval to get the necessary accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runge(x):\n",
    "    return 1.0 / (1 + 25 * x**2)\n",
    "\n",
    "result = scipy.integrate.quad(runge, 0, 2*np.pi, full_output=True)\n",
    "print(result[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
